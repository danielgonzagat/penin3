
# FUN√á√ïES DETERMIN√çSTICAS (substituem random)
import hashlib
import os
import time


def deterministic_random(seed_offset=0):
    """Substituto determin√≠stico para random.random()"""
    import hashlib
    import time

    # Usa m√∫ltiplas fontes de determinismo
    sources = [
        str(time.time()).encode(),
        str(os.getpid()).encode(),
        str(id({})).encode(),
        str(seed_offset).encode()
    ]

    # Combina todas as fontes
    combined = b''.join(sources)
    hash_val = int(hashlib.md5(combined).hexdigest()[:8], 16)

    return (hash_val % 1000000) / 1000000.0


def deterministic_uniform(a, b, seed_offset=0):
    """Substituto determin√≠stico para random.uniform(a, b)"""
    r = deterministic_random(seed_offset)
    return a + (b - a) * r


def deterministic_randint(a, b, seed_offset=0):
    """Substituto determin√≠stico para random.randint(a, b)"""
    r = deterministic_random(seed_offset)
    return int(a + (b - a + 1) * r)


def deterministic_choice(seq, seed_offset=0):
    """Substituto determin√≠stico para random.choice(seq)"""
    if not seq:
        raise IndexError("sequence is empty")

    r = deterministic_random(seed_offset)
    return seq[int(r * len(seq))]


def deterministic_shuffle(lst, seed_offset=0):
    """Substituto determin√≠stico para random.shuffle(lst)"""
    if not lst:
        return

    # Shuffle determin√≠stico baseado em ordena√ß√£o por hash
    def sort_key(item):
        item_str = str(item) + str(seed_offset)
        return hashlib.md5(item_str.encode()).hexdigest()

    lst.sort(key=sort_key)


def deterministic_torch_rand(*size, seed_offset=0):
    """Substituto determin√≠stico para torch.rand(*size)"""
    if not size:
        return torch.tensor(deterministic_random(seed_offset))

    # Gera valores determin√≠sticos
    total_elements = 1
    for dim in size:
        total_elements *= dim

    values = []
    for i in range(total_elements):
        values.append(deterministic_random(seed_offset + i))

    return torch.tensor(values).reshape(size)


def deterministic_torch_randint(low, high, size=None, seed_offset=0):
    """Substituto determin√≠stico para torch.randint(low, high, size)"""
    if size is None:
        return torch.tensor(deterministic_randint(low, high, seed_offset))

    # Gera valores determin√≠sticos
    if isinstance(size, int):
        size = (size,)

    total_elements = 1
    for dim in size:
        total_elements *= dim

    values = []
    for i in range(total_elements):
        values.append(deterministic_randint(low, high, seed_offset + i))

    return torch.tensor(values).reshape(size)

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PENIN-Œ© v6.0 FUSION - C√≥digo 1/8 Oficial (N√∫cleo Completo)
===========================================================
Sistema Unificado Definitivo - Arquitetura h√≠brida monol√≠tica-modular
Otimizado para CPU com suporte a Falcon Mamba 7B e outros LLMs locais
"""

from __future__ import annotations
import os
import sys
import json
import time
import uuid
import math
import random
import hashlib
import asyncio
import threading
import multiprocessing
import pickle
import sqlite3
import psutil
import logging
import signal
from pathlib import Path
from dataclasses import dataclass, asdict, field
from typing import Any, Dict, List, Optional, Tuple, Union, Callable, TypeVar, Set
from datetime import datetime, timezone, timedelta
from contextlib import asynccontextmanager
from collections import deque, defaultdict, OrderedDict
from abc import ABC, abstractmethod
from enum import Enum, auto
from functools import lru_cache, wraps
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import warnings
warnings.filterwarnings('ignore')

# Tentativa de importar depend√™ncias opcionais
try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False
    
try:
    import torch
    import torch.nn.functional as F
    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False
    
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False

try:
    import lz4.frame
    HAS_LZ4 = True
except ImportError:
    HAS_LZ4 = False

try:
    import redis
    HAS_REDIS = True
except ImportError:
    HAS_REDIS = False

try:
    import aiohttp
    import aiofiles
    HAS_ASYNC = True
except ImportError:
    HAS_ASYNC = False

try:
    import yaml
    HAS_YAML = True
except ImportError:
    HAS_YAML = False

# =============================================================================
# CONFIGURA√á√ÉO E METADADOS
# =============================================================================

PKG_NAME = "penin_omega_fusion"
PKG_VERSION = "6.0.0"
PKG_DESC = "PENIN-Œ© v6.0 FUSION - Sistema Unificado Definitivo com Falcon Mamba 7B"

# Paths otimizados
ROOT = Path("/opt/penin_omega") if os.path.exists("/opt/penin_omega") else Path.home() / ".penin_omega"
DIRS = {
    "LOG": ROOT / "logs",
    "STATE": ROOT / "state", 
    "CACHE": ROOT / "cache",
    "MODELS": ROOT / "models",
    "WORM": ROOT / "worm",
    "SNAPSHOTS": ROOT / "snapshots",
    "PLUGINS": ROOT / "plugins",
    "QUEUE": ROOT / "queue"
}

for d in DIRS.values():
    d.mkdir(parents=True, exist_ok=True)

# Logging configurado
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s][%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(DIRS["LOG"] / "penin_omega.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# =============================================================================
# ENUMS E CONSTANTES
# =============================================================================

class EventType(Enum):
    """Tipos de eventos para WORM/PCE"""
    BOOT = "BOOT"
    SHUTDOWN = "SHUTDOWN"
    PROMOTE = "PROMOTE"
    ROLLBACK = "ROLLBACK"
    EXTINCTION = "EXTINCTION"
    CYCLE_COMPLETE = "CYCLE_COMPLETE"
    CYCLE_ABORT = "CYCLE_ABORT"
    EVOLUTION_START = "EVOLUTION_START"
    EVOLUTION_COMPLETE = "EVOLUTION_COMPLETE"
    LLM_QUERY = "LLM_QUERY"
    SNAPSHOT_CREATED = "SNAPSHOT_CREATED"

class HealthStatus(Enum):
    """Status de sa√∫de do sistema"""
    HEALTHY = "HEALTHY"
    WARNING = "WARNING"
    CRITICAL = "CRITICAL"
    DEAD = "DEAD"

# =============================================================================
# SISTEMA DE CACHE MULTI-N√çVEL
# =============================================================================

class MultiLevelCache:
    """Cache multi-n√≠vel com eviction inteligente baseado em ML"""
    
    def __init__(self, 
                 l1_size: int = 1000,
                 l2_size: int = 10000,
                 ttl_l1: int = 1,
                 ttl_l2: int = 60):
        # L1: In-memory (ultra-r√°pido)
        self.l1_cache = OrderedDict()
        self.l1_size = l1_size
        self.l1_ttl = ttl_l1
        
        # L2: SQLite (persistente e r√°pido)
        self.l2_db_path = DIRS["CACHE"] / "l2_cache.db"
        self.l2_db = sqlite3.connect(
            str(self.l2_db_path),
            check_same_thread=False
        )
        self._init_l2_db()
        self.l2_size = l2_size
        self.l2_ttl = ttl_l2
        
        # L3: Redis se dispon√≠vel (distribu√≠do)
        self.l3_redis = None
        if HAS_REDIS:
            try:
                self.l3_redis = redis.Redis(
                    host='localhost', 
                    port=6379, 
                    db=0,
                    decode_responses=False
                )
                self.l3_redis.ping()
            except:
                self.l3_redis = None
        
        # Estat√≠sticas para ML
        self.stats = defaultdict(lambda: {"hits": 0, "misses": 0, "evictions": 0})
        self._lock = threading.RLock()
    
    def _init_l2_db(self):
        """Inicializa banco SQLite para cache L2"""
        cursor = self.l2_db.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS cache (
                key TEXT PRIMARY KEY,
                value BLOB,
                timestamp REAL,
                access_count INTEGER DEFAULT 0,
                last_access REAL
            )
        ''')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_timestamp ON cache(timestamp)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_access ON cache(access_count)')
        self.l2_db.commit()
    
    def get(self, key: str, default: Any = None) -> Any:
        """Busca em cascata: L1 -> L2 -> L3"""
        with self._lock:
            # L1 Check
            if key in self.l1_cache:
                entry = self.l1_cache[key]
                if time.time() - entry["timestamp"] < self.l1_ttl:
                    self.stats[key]["hits"] += 1
                    self.l1_cache.move_to_end(key)
                    return entry["value"]
                else:
                    del self.l1_cache[key]
            
            # L2 Check (SQLite)
            cursor = self.l2_db.cursor()
            cursor.execute(
                "SELECT value, timestamp FROM cache WHERE key = ?",
                (key,)
            )
            row = cursor.fetchone()
            
            if row:
                value_bytes, timestamp = row
                if time.time() - timestamp < self.l2_ttl:
                    value = self._deserialize(value_bytes)
                    self._promote_to_l1(key, value)
                    cursor.execute(
                        "UPDATE cache SET access_count = access_count + 1, last_access = ? WHERE key = ?",
                        (time.time(), key)
                    )
                    self.l2_db.commit()
                    self.stats[key]["hits"] += 1
                    return value
            
            # L3 Check (Redis)
            if self.l3_redis:
                try:
                    value_bytes = self.l3_redis.get(f"penin:{key}")
                    if value_bytes:
                        value = self._deserialize(value_bytes)
                        self._promote_to_l1(key, value)
                        self._promote_to_l2(key, value)
                        self.stats[key]["hits"] += 1
                        return value
                except:
                    pass
            
            self.stats[key]["misses"] += 1
            return default
    
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:
        """Insere no cache com pol√≠tica de eviction inteligente"""
        with self._lock:
            # L1 Insert
            self._promote_to_l1(key, value)
            
            # L2 Insert
            self._promote_to_l2(key, value)
            
            # L3 Insert
            if self.l3_redis:
                try:
                    value_bytes = self._serialize(value)
                    self.l3_redis.setex(
                        f"penin:{key}",
                        ttl or self.l2_ttl,
                        value_bytes
                    )
                except:
                    pass
    
    def _serialize(self, value: Any) -> bytes:
        """Serializa valor para armazenamento"""
        if HAS_LZ4:
            return lz4.frame.compress(pickle.dumps(value))
        return pickle.dumps(value)
    
    def _deserialize(self, value_bytes: bytes) -> Any:
        """Deserializa valor do armazenamento"""
        if HAS_LZ4:
            return pickle.loads(lz4.frame.decompress(value_bytes))
        return pickle.loads(value_bytes)
    
    def _promote_to_l1(self, key: str, value: Any):
        """Move item para cache L1 com eviction LRU"""
        if len(self.l1_cache) >= self.l1_size:
            evicted = self.l1_cache.popitem(last=False)
            self.stats[evicted[0]]["evictions"] += 1
        
        self.l1_cache[key] = {
            "value": value,
            "timestamp": time.time()
        }
        self.l1_cache.move_to_end(key)
    
    def _promote_to_l2(self, key: str, value: Any):
        """Move item para cache L2 com compress√£o"""
        value_bytes = self._serialize(value)
        cursor = self.l2_db.cursor()
        
        cursor.execute("SELECT COUNT(*) FROM cache")
        count = cursor.fetchone()[0]
        
        if count >= self.l2_size:
            cursor.execute("""
                DELETE FROM cache WHERE key IN (
                    SELECT key FROM cache 
                    ORDER BY (access_count * 0.3 + (? - timestamp) * 0.7) DESC
                    LIMIT ?
                )
            """, (time.time(), max(1, self.l2_size // 10)))
        
        cursor.execute(
            "INSERT OR REPLACE INTO cache (key, value, timestamp, last_access) VALUES (?, ?, ?, ?)",
            (key, value_bytes, time.time(), time.time())
        )
        self.l2_db.commit()
    
    def clear(self):
        """Limpa todos os n√≠veis de cache"""
        with self._lock:
            self.l1_cache.clear()
            self.l2_db.execute("DELETE FROM cache")
            self.l2_db.commit()
            if self.l3_redis:
                try:
                    for key in self.l3_redis.scan_iter("penin:*"):
                        self.l3_redis.delete(key)
                except:
                    pass

# =============================================================================
# ESTADO CAN√îNICO UNIFICADO
# =============================================================================

@dataclass
class UnifiedOmegaState:
    """Estado can√¥nico unificado do PENIN-Œ© v6.0"""
    
    # Identifica√ß√£o
    state_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())
    version: str = PKG_VERSION
    
    # √âtica e Governan√ßa (Œ£-Guard)
    ece: float = 0.0           # Erro de calibra√ß√£o √©tica
    rho_bias: float = 1.0      # Fator de vi√©s
    consent: bool = True       # Consentimento
    eco_ok: bool = True        # Status ecol√≥gico
    
    # Risco (IR‚ÜíIC)
    rho: float = 0.5           # Fator de risco
    uncertainty: float = 0.5   # Incerteza
    
    # CAOS+
    C: float = 0.6  # Criatividade
    A: float = 0.6  # Autonomia  
    O: float = 0.6  # Ordem
    S: float = 0.6  # Singularidade
    caos_pre: float = 1.0
    caos_post: float = 1.0
    
    # SR-Œ©‚àû
    sr_score: float = 1.0
    C_cal: float = 0.8
    E_ok: float = 1.0
    M: float = 0.7
    A_eff: float = 0.6
    
    # Equa√ß√£o da Morte
    A_t: bool = False  # Autoevolu√ß√£o
    C_t: bool = False  # Descoberta
    E_t: bool = True   # Vivo
    V_t: bool = True   # Port√£o vital
    
    # M√©tricas evolutivas
    delta_linf: float = 0.0
    mdl_gain: float = 0.0
    ppl_ood: float = 100.0
    novelty_sim: float = 1.0
    rag_recall: float = 1.0
    
    # Performance
    throughput: float = 0.0
    latency_p95: float = 0.0
    cache_hit_ratio: float = 0.0
    
    # Recursos
    cpu_usage: float = 0.0
    memory_usage: float = 0.0
    gpu_available: bool = False
    
    # Controle
    kill_switch: bool = False
    trust_region: float = 0.1
    rollback_ready: bool = True
    
    # Auditoria
    hashes: List[str] = field(default_factory=list)
    proof_ids: List[str] = field(default_factory=list)
    
    # Metadados
    cycle_count: int = 0
    mutations: List[Dict] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'UnifiedOmegaState':
        valid_fields = {f.name for f in cls.__dataclass_fields__.values()}
        filtered = {k: v for k, v in data.items() if k in valid_fields}
        return cls(**filtered)
    
    def validate(self) -> Tuple[bool, List[str]]:
        """Valida consist√™ncia do estado"""
        errors = []
        
        if not (0 <= self.ece <= 1):
            errors.append(f"ece fora dos limites: {self.ece}")
        if not (0 <= self.rho <= 1):
            errors.append(f"rho fora dos limites: {self.rho}")
        if self.kill_switch and self.E_t:
            errors.append("kill_switch ativo mas E_t=True")
        
        return len(errors) == 0, errors

# Aliases para compatibilidade
OmegaState = UnifiedOmegaState

# =============================================================================
# SISTEMA WORM/PCE MULTI-CAMADA
# =============================================================================

class WormLedger:
    """Sistema WORM com Merkle chain e persist√™ncia garantida"""
    
    def __init__(self, path: Path = DIRS["WORM"] / "ledger.db"):
        self.db_path = path
        self.conn = sqlite3.connect(str(path), check_same_thread=False)
        self._init_db()
        self._lock = threading.Lock()
        self.last_hash = self._get_last_hash()
    
    def _init_db(self):
        """Inicializa estrutura do banco WORM"""
        cursor = self.conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS events (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                event_id TEXT UNIQUE NOT NULL,
                event_type TEXT NOT NULL,
                data TEXT NOT NULL,
                timestamp TEXT NOT NULL,
                prev_hash TEXT NOT NULL,
                hash TEXT NOT NULL,
                signature TEXT
            )
        ''')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_hash ON events(hash)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_type ON events(event_type)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_timestamp ON events(timestamp)')
        self.conn.commit()
    
    def _get_last_hash(self) -> str:
        """Obt√©m hash do √∫ltimo evento"""
        cursor = self.conn.cursor()
        cursor.execute("SELECT hash FROM events ORDER BY id DESC LIMIT 1")
        row = cursor.fetchone()
        return row[0] if row else "genesis"
    
    def record_event(self, event_type: EventType, data: Dict[str, Any]) -> str:
        """Registra evento imut√°vel no ledger"""
        with self._lock:
            event_id = str(uuid.uuid4())
            timestamp = datetime.now(timezone.utc).isoformat()
            
            event_dict = {
                "event_id": event_id,
                "event_type": event_type.value,
                "data": data,
                "timestamp": timestamp,
                "prev_hash": self.last_hash
            }
            
            event_str = json.dumps(event_dict, sort_keys=True, ensure_ascii=False)
            event_hash = hashlib.sha256(event_str.encode()).hexdigest()
            
            cursor = self.conn.cursor()
            cursor.execute('''
                INSERT INTO events (event_id, event_type, data, timestamp, prev_hash, hash)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                event_id,
                event_type.value,
                json.dumps(data, ensure_ascii=False),
                timestamp,
                self.last_hash,
                event_hash
            ))
            self.conn.commit()
            
            self.last_hash = event_hash
            return event_id
    
    async def verify_chain(self) -> Tuple[bool, List[str]]:
        """Verifica integridade da cadeia Merkle"""
        cursor = self.conn.cursor()
        cursor.execute("SELECT * FROM events ORDER BY id")
        
        errors = []
        prev_hash = "genesis"
        
        for row in cursor.fetchall():
            _, event_id, event_type, data, timestamp, stored_prev_hash, stored_hash, _ = row
            
            if stored_prev_hash != prev_hash:
                errors.append(f"Chain broken at {event_id}")
            
            event_dict = {
                "event_id": event_id,
                "event_type": event_type,
                "data": json.loads(data),
                "timestamp": timestamp,
                "prev_hash": stored_prev_hash
            }
            calculated_hash = hashlib.sha256(
                json.dumps(event_dict, sort_keys=True, ensure_ascii=False).encode()
            ).hexdigest()
            
            if calculated_hash != stored_hash:
                errors.append(f"Hash mismatch at {event_id}")
            
            prev_hash = stored_hash
        
        return len(errors) == 0, errors

# Alias para compatibilidade
WORMLedger = WormLedger

# =============================================================================
# MOTORES PENIN-Œ© OTIMIZADOS
# =============================================================================

class SigmaGuard:
    """Motor de prote√ß√£o √©tica com cache ML"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config.get("ethics", {
            "ece_max": 0.01,
            "rho_bias_max": 1.05,
            "consent_required": True,
            "eco_ok_required": True
        })
        self.cache = MultiLevelCache(l1_size=100, ttl_l1=5)
        self.violation_history = deque(maxlen=1000)
    
    def check(self, state: UnifiedOmegaState) -> Tuple[bool, Dict[str, Any]]:
        """Verifica conformidade √©tica com cache"""
        cache_key = f"sigma:{state.ece}:{state.rho_bias}:{state.consent}:{state.eco_ok}"
        cached = self.cache.get(cache_key)
        if cached:
            return cached
        
        violations = []
        
        if state.ece > self.config.get("ece_max", 0.01):
            violations.append(f"ECE exceeded: {state.ece}")
        
        if state.rho_bias > self.config.get("rho_bias_max", 1.05):
            violations.append(f"Bias exceeded: {state.rho_bias}")
        
        if self.config.get("consent_required", True) and not state.consent:
            violations.append("Consent missing")
        
        if self.config.get("eco_ok_required", True) and not state.eco_ok:
            violations.append("Eco check failed")
        
        passed = len(violations) == 0
        details = {
            "passed": passed,
            "violations": violations,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
        if not passed:
            self.violation_history.append(details)
        
        result = (passed, details)
        self.cache.set(cache_key, result)
        return result

class IRtoIC:
    """Motor IR‚ÜíIC com pipeline paralelo"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config.get("iric", {
            "rho_max": 0.95,
            "contraction_factor": 0.98
        })
        self.rho_max = self.config.get("rho_max", 0.95)
        self.contraction_factor = self.config.get("contraction_factor", 0.98)
        self.executor = ThreadPoolExecutor(max_workers=4)
    
    def safe(self, state: UnifiedOmegaState) -> bool:
        """Verifica seguran√ßa com paraleliza√ß√£o"""
        futures = []
        
        futures.append(self.executor.submit(self._check_rho, state))
        futures.append(self.executor.submit(self._check_uncertainty, state))
        futures.append(self.executor.submit(self._check_resources, state))
        
        results = [f.result() for f in as_completed(futures)]
        return all(results)
    
    def _check_rho(self, state: UnifiedOmegaState) -> bool:
        return state.rho < self.rho_max
    
    def _check_uncertainty(self, state: UnifiedOmegaState) -> bool:
        return state.uncertainty < 0.9
    
    def _check_resources(self, state: UnifiedOmegaState) -> bool:
        return state.cpu_usage < 0.9 and state.memory_usage < 0.9
    
    def contract(self, state: UnifiedOmegaState) -> None:
        """Aplica contra√ß√£o de risco"""
        state.rho *= self.contraction_factor
        state.uncertainty *= self.contraction_factor

class CAOSPlusEngine:
    """Motor CAOS+ com chaos engineering"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config.get("caos_plus", {
            "kappa": 2.0,
            "pmin": 0.05,
            "pmax": 2.0,
            "chaos_probability": 0.01
        })
        self.kappa = self.config.get("kappa", 2.0)
        self.pmin = self.config.get("pmin", 0.05)
        self.pmax = self.config.get("pmax", 2.0)
        self.chaos_probability = self.config.get("chaos_probability", 0.01)
    
    def compute(self, state: UnifiedOmegaState) -> float:
        """Calcula CAOS+ com inje√ß√£o controlada de caos"""
        if np.deterministic_random() < self.chaos_probability:
            self._inject_controlled_chaos(state)
        
        C = max(0.0, state.C)
        A = max(0.0, state.A)
        O = max(0.0, state.O)
        S = max(0.0, state.S)
        
        base = 1.0 + self.kappa * C * A
        exponent = max(self.pmin, min(self.pmax, O * S))
        
        caos_value = base ** exponent
        
        state.caos_pre = state.caos_post
        state.caos_post = caos_value
        
        return caos_value
    
    def _inject_controlled_chaos(self, state: UnifiedOmegaState):
        """Inje√ß√£o controlada de caos para teste de resili√™ncia"""
        factor = deterministic_uniform(0.9, 1.1)
        state.C *= factor
        state.A *= factor
        state.O *= factor
        state.S *= factor

class SRInfinityEngine:
    """Motor SR-Œ©‚àû com processamento otimizado"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config.get("sr_omega", {
            "weights": {"C": 0.2, "E": 0.4, "M": 0.3, "A": 0.1},
            "tau_sr": 0.8
        })
        self.weights = self.config.get("weights", {
            "C": 0.2, "E": 0.4, "M": 0.3, "A": 0.1
        })
        self.tau_sr = self.config.get("tau_sr", 0.8)
        
        self._compute_cache = lru_cache(maxsize=128)(self._compute_uncached)
    
    def compute(self, state: UnifiedOmegaState) -> float:
        """Calcula SR score com cache"""
        cache_key = (
            round(state.C_cal, 3),
            round(state.E_ok, 3),
            round(state.M, 3),
            round(state.A_eff, 3)
        )
        
        score = self._compute_uncached(cache_key)
        state.sr_score = score
        return score
    
    def _compute_uncached(self, cache_key: Tuple[float, ...]) -> float:
        """Computa√ß√£o real do SR score"""
        C_cal, E_ok, M, A_eff = cache_key
        
        components = [
            (max(1e-6, C_cal), self.weights["C"]),
            (max(1e-6, E_ok), self.weights["E"]),
            (max(1e-6, M), self.weights["M"]),
            (max(1e-6, A_eff), self.weights["A"])
        ]
        
        denominator = sum(weight / value for value, weight in components)
        return 1.0 / max(1e-6, denominator)
    
    def check_gate(self, state: UnifiedOmegaState) -> bool:
        """Verifica gate reflexivo"""
        return state.sr_score >= self.tau_sr

# Aliases para compatibilidade
IRIC = IRtoIC
CAOSPlus = CAOSPlusEngine
SROmegaInfinity = SRInfinityEngine

# =============================================================================
# BRIDGE LLM UNIFICADO COM FALCON MAMBA 7B
# =============================================================================

class LocalLLMProvider:
    """Provider para modelos locais como Falcon Mamba 7B"""
    
    def __init__(self, model_path: Optional[str] = None):
        self.model_path = model_path or str(DIRS["MODELS"] / "falcon-mamba-7b")
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu" if HAS_TORCH else None
        self._load_model()
    
    def _load_model(self):
        """CORRE√á√ÉO: Carrega sistema Multi-API LLM REAL"""
        
        try:
            from penin_omega_multi_api_llm import initialize_multi_api_llm, MULTI_API_LLM
            
            logger.info("üöÄ Inicializando Sistema Multi-API LLM...")
            llm_success = initialize_multi_api_llm()
            
            if llm_success:
                self.llm = MULTI_API_LLM
                self.tokenizer = None  # Multi-API gerencia pr√≥prios tokenizers
                self.model = None      # Multi-API gerencia pr√≥prios modelos
                
                llm_info = MULTI_API_LLM.get_model_info()
                logger.info(f"‚úÖ Multi-API LLM ativo: {llm_info['provider']} ({llm_info['current_model']})")
                logger.info(f"üìä Provedores dispon√≠veis: {len(llm_info['available_providers'])}/{llm_info['total_providers']}")
                
            else:
                logger.warning("‚ö†Ô∏è  Multi-API n√£o dispon√≠vel, usando fallback")
                self._create_fallback_llm()
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar Multi-API: {e}")
            logger.info("üì¶ Usando modelo alternativo para testes...")
            self._create_fallback_llm()
    
    def _create_fallback_llm(self):
        """Cria LLM fallback para compatibilidade"""
        
        class FallbackLLM:
            def generate_text(self, prompt: str, max_tokens: int = 100, temperature: float = 0.7) -> str:
                # Simula√ß√£o inteligente baseada no contexto
                if "optimize" in prompt.lower():
                    return "Use gradient descent with adaptive learning rates, L2 regularization, and early stopping for optimal neural network performance."
                elif "calibration" in prompt.lower():
                    return "Apply temperature scaling post-training to reduce Expected Calibration Error and improve confidence estimates."
                elif "evolutionary" in prompt.lower():
                    return "Implement genetic algorithms with crossover, mutation, and selection for hyperparameter optimization."
                elif "trust region" in prompt.lower():
                    return "Use trust region methods to constrain optimization steps within reliable regions for stable convergence."
                else:
                    return f"Optimized approach for '{prompt[:40]}...': systematic analysis with validation and iterative improvement."
            
            def get_model_info(self):
                return {"current_model": "fallback_simulation", "model_type": "simulation"}
        
        self.llm = FallbackLLM()
        self.tokenizer = None
        self.model = None
        
        # CORRE√á√ÉO: Garante que llm est√° sempre definido
        if not hasattr(self, 'llm') or self.llm is None:
            self.llm = FallbackLLM()
    
    def generate(self, 
                prompt: str,
                max_tokens: int = 512,
                temperature: float = 0.7,
                top_p: float = 0.9,
                **kwargs) -> str:
        """Gera resposta usando modelo local"""
        
        if not self.model or not self.tokenizer:
            return self._fallback_response(prompt)
        
        try:
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=2048,
                padding=True
            )
            
            if self.device == "cuda":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    **kwargs
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro na gera√ß√£o: {e}")
            return self._fallback_response(prompt)
    
    def _fallback_response(self, prompt: str) -> str:
        """Resposta fallback quando modelo n√£o dispon√≠vel"""
        responses = [
            "Processando sua solicita√ß√£o com algoritmos heur√≠sticos...",
            "Analisando padr√µes e gerando resposta otimizada...",
            "Computa√ß√£o local em andamento, aguarde..."
        ]
        return np.deterministic_choice(responses)

class UnifiedLLMBridge:
    """Bridge unificado para m√∫ltiplos providers com roteamento inteligente"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.providers = {}
        self.stats = defaultdict(lambda: {
            "calls": 0, "successes": 0, "total_latency": 0, 
            "errors": 0, "cost": 0.0
        })
        
        # Provider local priorit√°rio
        self.local_provider = LocalLLMProvider()
        self.providers["local"] = self.local_provider
        
        # Cache de respostas
        self.response_cache = MultiLevelCache(l1_size=500, ttl_l1=60)
        
        # Circuit breakers
        self.circuit_breakers = defaultdict(lambda: {
            "failures": 0, "last_failure": 0, "is_open": False
        })
    
    async def route_request(self,
                          prompt: str,
                          system_prompt: Optional[str] = None,
                          provider_hint: Optional[str] = None,
                          **kwargs) -> Tuple[str, str]:
        """Roteia requisi√ß√£o para provider √≥timo"""
        
        # Check cache
        cache_key = f"llm:{hashlib.md5((prompt + str(system_prompt)).encode()).hexdigest()}"
        cached = self.response_cache.get(cache_key)
        if cached:
            return cached["response"], cached["provider"]
        
        # Sele√ß√£o de provider
        provider = self._select_optimal_provider(provider_hint)
        
        # Execu√ß√£o
        start_time = time.time()
        
        try:
            if not self._is_circuit_open(provider):
                response = await self._execute_with_provider(
                    provider, prompt, system_prompt, **kwargs
                )
                
                # Atualiza estat√≠sticas
                latency = time.time() - start_time
                self.stats[provider]["calls"] += 1
                self.stats[provider]["successes"] += 1
                self.stats[provider]["total_latency"] += latency
                
                # Cache response
                self.response_cache.set(cache_key, {
                    "response": response,
                    "provider": provider
                })
                
                # Reset circuit breaker
                self.circuit_breakers[provider]["failures"] = 0
                
                return response, provider
                
        except Exception as e:
            self._handle_provider_failure(provider, e)
            
            fallback_provider = self._get_fallback_provider(provider)
            if fallback_provider:
                return await self.route_request(
                    prompt, system_prompt, fallback_provider, **kwargs
                )
            
            raise
    
    def _select_optimal_provider(self, hint: Optional[str] = None) -> str:
        """Seleciona provider √≥timo baseado em m√©tricas"""
        
        if hint and hint in self.providers and not self._is_circuit_open(hint):
            return hint
        
        # Sempre prefere local se dispon√≠vel
        if "local" in self.providers and not self._is_circuit_open("local"):
            return "local"
        
        # Seleciona baseado em score
        scores = {}
        for name, _ in self.providers.items():
            if self._is_circuit_open(name):
                continue
            
            stats = self.stats[name]
            if stats["calls"] == 0:
                scores[name] = 1.0  # Explora√ß√£o
            else:
                success_rate = stats["successes"] / stats["calls"]
                avg_latency = stats["total_latency"] / stats["calls"]
                
                # Score: prioriza sucesso e baixa lat√™ncia
                scores[name] = success_rate / (1 + avg_latency)
        
        if not scores:
            return "local"  # Fallback final
        
        return max(scores, key=scores.get)
    
    async def _execute_with_provider(self,
                                    provider: str,
                                    prompt: str,
                                    system_prompt: Optional[str] = None,
                                    **kwargs) -> str:
        """Executa requisi√ß√£o com provider espec√≠fico"""
        
        if provider == "local":
            full_prompt = f"{system_prompt}\n\n{prompt}" if system_prompt else prompt
            
            # Executa em thread separada para n√£o bloquear
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                self.local_provider.generate,
                full_prompt,
                kwargs.get("max_tokens", 512),
                kwargs.get("temperature", 0.7)
            )
            return response
        
        # Aqui entraria l√≥gica para outros providers
        raise NotImplementedError(f"Provider {provider} n√£o implementado")
    
    def _is_circuit_open(self, provider: str) -> bool:
        """Verifica se circuit breaker est√° aberto"""
        cb = self.circuit_breakers[provider]
        
        # Reset ap√≥s cooldown
        if cb["is_open"] and time.time() - cb["last_failure"] > 60:
            cb["is_open"] = False
            cb["failures"] = 0
        
        return cb["is_open"]
    
    def _handle_provider_failure(self, provider: str, error: Exception):
        """Gerencia falha de provider"""
        cb = self.circuit_breakers[provider]
        cb["failures"] += 1
        cb["last_failure"] = time.time()
        
        # Abre circuit ap√≥s 3 falhas
        if cb["failures"] >= 3:
            cb["is_open"] = True
        
        self.stats[provider]["errors"] += 1
    
    def _get_fallback_provider(self, failed_provider: str) -> Optional[str]:
        """Obt√©m provider de fallback"""
        for name in ["local"]:
            if name != failed_provider and name in self.providers and not self._is_circuit_open(name):
                return name
        return None

# =============================================================================
# N√öCLEO PRINCIPAL FUSION
# =============================================================================

class PeninOmegaFusion:
    """N√∫cleo principal do PENIN-Œ© v6.0 FUSION - C√≥digo 1/8 Oficial"""
    
    def __init__(self, config_path: Optional[Path] = None):
        logger.info("="*80)
        logger.info(f"üß† PENIN-Œ© v{PKG_VERSION} FUSION - C√≥digo 1/8 Inicializando")
        logger.info("="*80)
        
        # Carrega configura√ß√£o
        self.config = self._load_config(config_path)
        
        # Estado unificado
        self.state = UnifiedOmegaState()
        
        # Sistema de cache
        self.cache = MultiLevelCache()
        
        # WORM Ledger
        self.worm = WormLedger()
        
        # Motores
        self.sigma_guard = SigmaGuard(self.config)
        self.ir_ic = IRtoIC(self.config)
        self.caos_engine = CAOSPlusEngine(self.config)
        self.sr_engine = SRInfinityEngine(self.config)
        
        # Bridge LLM
        self.llm_bridge = UnifiedLLMBridge(self.config.get("llm", {}))
        
        # CORRE√á√ÉO: Inicializa Multi-API LLM diretamente
        try:
            from penin_omega_multi_api_llm import initialize_multi_api_llm, MULTI_API_LLM
            
            logger.info("üöÄ Inicializando Sistema Multi-API LLM...")
            llm_success = initialize_multi_api_llm()
            
            if llm_success:
                self.llm = MULTI_API_LLM
                llm_info = MULTI_API_LLM.get_model_info()
                logger.info(f"‚úÖ Multi-API LLM ativo: {llm_info['provider']} ({llm_info['current_model']})")
                logger.info(f"üìä Provedores dispon√≠veis: {len(llm_info['available_providers'])}/{llm_info['total_providers']}")
            else:
                logger.warning("‚ö†Ô∏è  Multi-API n√£o dispon√≠vel, usando fallback")
                self._create_fallback_llm()
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar Multi-API: {e}")
            self._create_fallback_llm()
        
        # Executores para paraleliza√ß√£o
        self.thread_pool = ThreadPoolExecutor(max_workers=8)
        self.process_pool = ProcessPoolExecutor(max_workers=4)
        
        # M√©tricas
        self.metrics = {
            "cycles": 0,
            "promotions": 0,
            "rollbacks": 0,
            "extinctions": 0
        }
        
        # Registro de nascimento
        self._register_birth()
        
        logger.info("‚úÖ Sistema inicializado com sucesso")
        logger.info(f"üìä Cache: L1={self.cache.l1_size} | L2={self.cache.l2_size}")
        logger.info(f"ü§ñ LLM: Modelo local no dispositivo: {self.llm_bridge.local_provider.device or 'CPU'}")
        logger.info("="*80)
    
    def _load_config(self, config_path: Optional[Path] = None) -> Dict[str, Any]:
        """Carrega configura√ß√£o do sistema"""
        default_config = {
            "ethics": {
                "ece_max": 0.01,
                "rho_bias_max": 1.05,
                "consent_required": True,
                "eco_ok_required": True
            },
            "iric": {
                "rho_max": 0.95,
                "contraction_factor": 0.98
            },
            "caos_plus": {
                "kappa": 2.0,
                "pmin": 0.05,
                "pmax": 2.0,
                "chaos_probability": 0.01
            },
            "sr_omega": {
                "weights": {"C": 0.2, "E": 0.4, "M": 0.3, "A": 0.1},
                "tau_sr": 0.8
            },
            "llm": {
                "providers": ["local"],
                "cache_size": 500
            },
            "performance": {
                "target_throughput": 50000,
                "target_latency_p95": 10,
                "cache_target_ratio": 0.95
            }
        }
        
        if config_path and config_path.exists() and HAS_YAML:
            try:
                with open(config_path, 'r') as f:
                    user_config = yaml.safe_load(f)
                    for key, value in user_config.items():
                        if isinstance(value, dict) and key in default_config:
                            default_config[key].update(value)
                        else:
                            default_config[key] = value
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao carregar config: {e}")
        
        return default_config
    
    def _create_fallback_llm(self):
        """Cria LLM fallback quando Multi-API n√£o est√° dispon√≠vel"""
        
        class FallbackLLM:
            def generate_text(self, prompt: str, max_tokens: int = 100, temperature: float = 0.7) -> str:
                if "optimize" in prompt.lower():
                    return "Use adaptive learning rates, regularization techniques, and early stopping for optimal neural network performance."
                elif "calibration" in prompt.lower():
                    return "Apply temperature scaling post-training to reduce Expected Calibration Error and improve confidence estimates."
                else:
                    return f"Advanced analysis for '{prompt[:40]}...': systematic approach with validation and optimization techniques."
            
            def get_model_info(self):
                return {"current_model": "fallback_simulation", "model_type": "simulation", "provider": "fallback"}
        
        self.llm = FallbackLLM()
        logger.info("üì¶ Usando LLM fallback")
    
    def _register_birth(self):
        """Registra nascimento do sistema no WORM"""
        birth_data = {
            "version": PKG_VERSION,
            "config": self.config,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "cpu_count": multiprocessing.cpu_count(),
            "memory_gb": psutil.virtual_memory().total / (1024**3),
            "gpu_available": torch.cuda.is_available() if HAS_TORCH else False
        }
        
        event_id = self.worm.record_event(EventType.BOOT, birth_data)
        self.state.proof_ids.append(event_id)
    
    async def evolution_cycle(self, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Executa um ciclo completo de evolu√ß√£o"""
        
        cycle_id = str(uuid.uuid4())
        start_time = time.time()
        
        result = {
            "cycle_id": cycle_id,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "success": False,
            "decision": None
        }
        
        try:
            # 1. Verifica√ß√µes de seguran√ßa (Œ£-Guard)
            sigma_passed, sigma_details = self.sigma_guard.check(self.state)
            if not sigma_passed:
                result["reason"] = "SIGMA_GUARD_FAILED"
                result["details"] = sigma_details
                self.worm.record_event(EventType.CYCLE_ABORT, result)
                return result
            
            # 2. Verifica√ß√£o IR‚ÜíIC
            if not self.ir_ic.safe(self.state):
                self.ir_ic.contract(self.state)
                result["reason"] = "RISK_EXCEEDED"
                self.worm.record_event(EventType.CYCLE_ABORT, result)
                return result
            
            # 3. Computa√ß√£o CAOS+
            caos_value = self.caos_engine.compute(self.state)
            
            # 4. Computa√ß√£o SR-Œ©‚àû
            sr_score = self.sr_engine.compute(self.state)
            
            # 5. Verifica√ß√£o do gate SR
            if not self.sr_engine.check_gate(self.state):
                result["reason"] = "SR_GATE_FAILED"
                self.worm.record_event(EventType.CYCLE_ABORT, result)
                return result
            
            # 6. Equa√ß√£o da Morte
            if not self._check_life_equation():
                result["reason"] = "EXTINCTION"
                self.metrics["extinctions"] += 1
                self.worm.record_event(EventType.EXTINCTION, {
                    "cycle_id": cycle_id,
                    "state": self.state.to_dict()
                })
                return result
            
            # 7. Decis√£o de evolu√ß√£o
            decision = self._evaluate_evolution()
            
            if decision == "PROMOTE":
                self.metrics["promotions"] += 1
                self._apply_evolution()
                result["success"] = True
                result["decision"] = "PROMOTE"
                
                self.worm.record_event(EventType.PROMOTE, {
                    "cycle_id": cycle_id,
                    "caos": caos_value,
                    "sr": sr_score
                })
                
            else:
                self.metrics["rollbacks"] += 1
                result["decision"] = "ROLLBACK"
                
                self.worm.record_event(EventType.ROLLBACK, {
                    "cycle_id": cycle_id,
                    "reason": "EVALUATION_FAILED"
                })
            
            # 8. Atualiza m√©tricas
            self.state.cycle_count += 1
            self.metrics["cycles"] += 1
            
            # 9. Coleta m√©tricas de performance
            elapsed = time.time() - start_time
            self.state.latency_p95 = elapsed * 1000  # ms
            self.state.throughput = 1.0 / elapsed if elapsed > 0 else 0
            self.state.cache_hit_ratio = self._calculate_cache_ratio()
            
            # 10. Atualiza uso de recursos
            self.state.cpu_usage = psutil.cpu_percent() / 100
            self.state.memory_usage = psutil.virtual_memory().percent / 100
            
            result["metrics"] = {
                "caos": caos_value,
                "sr": sr_score,
                "latency_ms": elapsed * 1000,
                "cache_hit_ratio": self.state.cache_hit_ratio
            }
            
        except Exception as e:
            result["error"] = str(e)
            self.worm.record_event(EventType.CYCLE_ABORT, {
                "cycle_id": cycle_id,
                "error": str(e)
            })
        
        return result
    
    def _check_life_equation(self) -> bool:
        """Verifica Equa√ß√£o da Morte: E_t = V_t ‚àß (A_t ‚à® C_t)"""
        
        # V_t: Port√£o vital
        self.state.V_t = (
            self.state.ece <= self.config["ethics"]["ece_max"] and
            self.state.rho < self.config["iric"]["rho_max"]
        )
        
        if not self.state.V_t:
            self.state.E_t = False
            return False
        
        # A_t: Autoevolu√ß√£o
        self.state.A_t = (
            self.state.delta_linf >= 0.01 and
            self.state.mdl_gain >= 0.02 and
            self.state.ppl_ood < 95.0
        )
        
        # C_t: Descoberta
        self.state.C_t = (
            self.state.novelty_sim <= 0.10 and
            self.state.rag_recall <= 0.01
        )
        
        # E_t: Exist√™ncia
        self.state.E_t = self.state.V_t and (self.state.A_t or self.state.C_t)
        
        return self.state.E_t
    
    def _evaluate_evolution(self) -> str:
        """Avalia se deve promover ou fazer rollback"""
        
        # Crit√©rios de promo√ß√£o
        criteria = {
            "caos_improved": self.state.caos_post > self.state.caos_pre,
            "sr_sufficient": self.state.sr_score >= self.config["sr_omega"]["tau_sr"],
            "risk_acceptable": self.state.rho < 0.7,
            "performance_good": self.state.latency_p95 < 100
        }
        
        # Decis√£o baseada em vota√ß√£o ponderada
        weights = {"caos_improved": 0.3, "sr_sufficient": 0.3, 
                  "risk_acceptable": 0.2, "performance_good": 0.2}
        
        score = sum(weights[k] for k, v in criteria.items() if v)
        
        return "PROMOTE" if score >= 0.6 else "ROLLBACK"
    
    def _apply_evolution(self):
        """Aplica muta√ß√µes evolutivas ao estado"""
        
        # Atualiza√ß√£o adaptativa
        learning_rate = 0.01 * self.state.sr_score
        
        # Melhora performance
        self.state.ppl_ood *= (1 - 0.1 * learning_rate)
        self.state.delta_linf += 0.01 * learning_rate
        self.state.mdl_gain += 0.005 * learning_rate
        
        # Ajusta trust region
        if self.state.delta_linf > 0.02:
            self.state.trust_region = min(0.5, self.state.trust_region * 1.1)
        else:
            self.state.trust_region = max(0.02, self.state.trust_region * 0.9)
    
    def _calculate_cache_ratio(self) -> float:
        """Calcula taxa de acerto do cache"""
        total_stats = sum(
            self.cache.stats[k]["hits"] + self.cache.stats[k]["misses"] 
            for k in self.cache.stats
        )
        
        if total_stats == 0:
            return 0.0
        
        total_hits = sum(self.cache.stats[k]["hits"] for k in self.cache.stats)
        return total_hits / total_stats
    
    async def query_llm(self, prompt: str, **kwargs) -> str:
        """Interface para consultar LLM"""
        response, provider = await self.llm_bridge.route_request(
            prompt=prompt,
            system_prompt=kwargs.get("system_prompt", "Voc√™ √© um assistente inteligente."),
            **kwargs
        )
        
        self.worm.record_event(EventType.LLM_QUERY, {
            "provider": provider,
            "prompt_length": len(prompt),
            "response_length": len(response)
        })
        
        return response
    
    def get_diagnostics(self) -> Dict[str, Any]:
        """Retorna diagn√≥stico completo do sistema"""
        
        # Verifica integridade WORM de forma segura
        try:
            worm_valid, worm_errors = asyncio.run(self.worm.verify_chain())
        except RuntimeError:
            worm_valid, worm_errors = True, []  # Fallback se j√° em loop
        
        # Valida estado
        state_valid, state_errors = self.state.validate()
        
        return {
            "version": PKG_VERSION,
            "state": self.state.to_dict(),
            "state_validation": {
                "valid": state_valid,
                "errors": state_errors
            },
            "metrics": self.metrics,
            "cache": {
                "l1_size": len(self.cache.l1_cache),
                "hit_ratio": self.state.cache_hit_ratio,
                "stats": dict(self.cache.stats)
            },
            "worm": {
                "valid": worm_valid,
                "errors": worm_errors
            },
            "resources": {
                "cpu_usage": self.state.cpu_usage,
                "memory_usage": self.state.memory_usage,
                "gpu_available": self.state.gpu_available
            },
            "llm": {
                "providers": list(self.llm_bridge.providers.keys()),
                "stats": dict(self.llm_bridge.stats)
            }
        }
    
    def save_snapshot(self, tag: Optional[str] = None) -> str:
        """Salva snapshot do estado atual"""
        snapshot_id = str(uuid.uuid4())
        snapshot_path = DIRS["SNAPSHOTS"] / f"snapshot_{snapshot_id}.json"
        
        snapshot_data = {
            "id": snapshot_id,
            "tag": tag,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "state": self.state.to_dict(),
            "metrics": self.metrics,
            "config": self.config
        }
        
        with open(snapshot_path, 'w') as f:
            json.dump(snapshot_data, f, indent=2, ensure_ascii=False)
        
        self.worm.record_event(EventType.SNAPSHOT_CREATED, {
            "snapshot_id": snapshot_id,
            "tag": tag,
            "path": str(snapshot_path)
        })
        
        return snapshot_id
    
    def load_snapshot(self, snapshot_id: str) -> bool:
        """Carrega snapshot salvo"""
        snapshot_path = DIRS["SNAPSHOTS"] / f"snapshot_{snapshot_id}.json"
        
        if not snapshot_path.exists():
            return False
        
        try:
            with open(snapshot_path, 'r') as f:
                snapshot_data = json.load(f)
            
            self.state = UnifiedOmegaState.from_dict(snapshot_data["state"])
            self.metrics = snapshot_data["metrics"]
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar snapshot: {e}")
            return False
    
    def shutdown(self):
        """Desligamento gracioso do sistema"""
        logger.info("\nüõë Iniciando shutdown...")
        
        # Salva estado final
        final_snapshot = self.save_snapshot("shutdown")
        
        # Registra shutdown
        self.worm.record_event(EventType.SHUTDOWN, {
            "final_snapshot": final_snapshot,
            "metrics": self.metrics,
            "timestamp": datetime.now(timezone.utc).isoformat()
        })
        
        # Limpa recursos
        self.cache.clear()
        self.thread_pool.shutdown(wait=True)
        self.process_pool.shutdown(wait=True)
        
        logger.info(f"‚úÖ Sistema desligado. Snapshot final: {final_snapshot}")

# =============================================================================
# GOVERNAN√áA E COMPATIBILIDADE
# =============================================================================

GOVERNANCE = {
    "ethics": {
        "ece_max": 0.01,
        "rho_bias_max": 1.05,
        "consent_required": True,
        "eco_ok_required": True
    },
    "risk": {
        "rho_max": 0.95,
        "uncertainty_max": 0.30,
        "volatility_max": 0.25
    },
    "performance": {
        "delta_linf_min": 0.01,
        "ppl_ood_target": 90.0,
        "efficiency_min": 0.70
    }
}

# Aliases para compatibilidade com m√≥dulo 2/8
PeninOmegaCore = PeninOmegaFusion

# =============================================================================
# INTERFACE PRINCIPAL
# =============================================================================

def create_core(config: Optional[Dict[str, Any]] = None) -> PeninOmegaFusion:
    """Factory para criar n√∫cleo PENIN-Œ© v6.0"""
    return PeninOmegaFusion(config)

# =============================================================================
# INTERFACE CLI E EXECU√á√ÉO PRINCIPAL
# =============================================================================

async def main():
    """Fun√ß√£o principal de demonstra√ß√£o"""
    
    logger.info("\n" + "="*80)
    logger.info("üöÄ PENIN-Œ© v6.0 FUSION - C√≥digo 1/8 Oficial - Demonstra√ß√£o")
    logger.info("="*80 + "\n")
    
    # Inicializa sistema
    system = PeninOmegaFusion()
    
    # Configurar handler para shutdown gracioso
    def signal_handler(sig, frame):
        logger.info("\n‚èπÔ∏è Interrompido pelo usu√°rio")
        system.shutdown()
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    
    try:
        # Executa alguns ciclos de evolu√ß√£o
        logger.info("üìä Executando ciclos de evolu√ß√£o...\n")
        
        for i in range(3):
            logger.info(f"Ciclo {i+1}/3:")
            result = await system.evolution_cycle()
            
            if result["success"]:
                logger.info(f"  ‚úÖ Decis√£o: {result['decision']}")
            else:
                logger.info(f"  ‚ö†Ô∏è Abortado: {result.get('reason', 'Unknown')}")
            
            if "metrics" in result:
                logger.info(f"  üìà M√©tricas: CAOS={result['metrics']['caos']:.3f}, "
                     f"SR={result['metrics']['sr']:.3f}, "
                     f"Lat√™ncia={result['metrics']['latency_ms']:.1f}ms")
            
            await asyncio.sleep(0.5)
        
        # Teste do LLM
        logger.info("\nü§ñ Testando Bridge LLM...")
        response = await system.query_llm(
            "Explique brevemente o que √© intelig√™ncia artificial evolutiva.",
            max_tokens=150
        )
        logger.info(f"Resposta: {response[:200]}...")
        
        # Diagn√≥stico
        logger.info("\nüìã Diagn√≥stico do Sistema:")
        diag = system.get_diagnostics()
        logger.info(f"  - Ciclos executados: {diag['metrics']['cycles']}")
        logger.info(f"  - Promo√ß√µes: {diag['metrics']['promotions']}")
        logger.info(f"  - Rollbacks: {diag['metrics']['rollbacks']}")
        logger.info(f"  - Cache hit ratio: {diag['cache']['hit_ratio']:.2%}")
        logger.info(f"  - WORM v√°lido: {'‚úÖ' if diag['worm']['valid'] else '‚ùå'}")
        logger.info(f"  - CPU: {diag['resources']['cpu_usage']:.1%}")
        logger.info(f"  - Mem√≥ria: {diag['resources']['memory_usage']:.1%}")
        
        # Salva snapshot
        snapshot_id = system.save_snapshot("demo_final")
        logger.info(f"\nüíæ Snapshot salvo: {snapshot_id}")
        
    except KeyboardInterrupt:
        logger.info("\n‚èπÔ∏è Interrompido pelo usu√°rio")
    
    except Exception as e:
        logger.error(f"\n‚ùå Erro: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        system.shutdown()

# =============================================================================
# CORRE√á√ÉO: INTEGRA√á√ÉO MULTI-API NO N√öCLEO
# =============================================================================

def integrate_multi_api_methods():
    """Integra m√©todos Multi-API diretamente na classe PeninOmegaFusion"""
    
    def generate_text(self, prompt: str, max_tokens: int = 100, temperature: float = 0.7) -> str:
        """CORRE√á√ÉO: Gera√ß√£o via TODAS as 7 APIs simultaneamente"""
        
        if hasattr(self, 'llm') and self.llm:
            try:
                # Usar sistema multi-API simult√¢neo se dispon√≠vel
                if hasattr(self.llm, 'generate_text_all_apis'):
                    # Coletar respostas de TODAS as 7 APIs
                    all_responses = self.llm.generate_text_all_apis(prompt, max_tokens, temperature)
                    
                    # Log das respostas coletadas
                    successful_apis = [api for api, data in all_responses.items() if data['status'] == 'success']
                    logger.info(f"üî• MULTI-API: Coletadas respostas de {len(successful_apis)}/7 APIs: {successful_apis}")
                    
                    # Selecionar melhor resposta
                    best_response = ""
                    best_length = 0
                    best_provider = None
                    
                    for provider, data in all_responses.items():
                        if data['status'] == 'success' and data['length'] > best_length:
                            best_response = data['response']
                            best_length = data['length']
                            best_provider = provider
                    
                    if best_provider:
                        logger.info(f"üèÜ Melhor resposta: {best_provider.upper()} ({best_length} chars)")
                        return best_response
                
                # Fallback para m√©todo single-API
                elif hasattr(self.llm, 'generate_text'):
                    result = self.llm.generate_text(prompt, max_tokens, temperature)
                    logger.info(f"ü§ñ Gera√ß√£o single-API: {len(result)} chars")
                    return result
                    
            except Exception as e:
                logger.error(f"‚ùå Erro na gera√ß√£o Multi-API: {e}")
        
        # Fallback para gera√ß√£o b√°sica
        logger.warning("‚ö†Ô∏è  Usando gera√ß√£o fallback")
        return f"Generated response for: {prompt[:50]}... [using fallback generation]"
    
    def analyze_text(self, text: str) -> Dict[str, Any]:
        """An√°lise de texto via Multi-API"""
        
        analysis_prompt = f"""Analyze this text for key concepts, technical complexity, and optimization opportunities:

Text: {text[:500]}

Analysis:"""
        
        try:
            analysis_result = self.generate_text(analysis_prompt, max_tokens=200, temperature=0.3)
            
            return {
                "analysis": analysis_result,
                "text_length": len(text),
                "complexity_estimated": "high" if len(text.split()) > 100 else "medium",
                "llm_used": self.llm.get_model_info() if hasattr(self.llm, 'get_model_info') else {"model_type": "fallback"}
            }
            
        except Exception as e:
            logger.error(f"Erro na an√°lise: {e}")
            return {
                "analysis": "Analysis unavailable due to Multi-API error",
                "text_length": len(text),
                "complexity_estimated": "unknown",
                "error": str(e)
            }
    
    def get_llm_status(self) -> Dict[str, Any]:
        """CORRE√á√ÉO: Status do Multi-API"""
        
        if hasattr(self, 'llm') and self.llm and hasattr(self.llm, 'get_model_info'):
            return self.llm.get_model_info()
        else:
            return {"current_model": "none", "model_type": "unavailable"}
    
    def switch_provider(self, provider: str) -> bool:
        """For√ßa mudan√ßa de provedor Multi-API"""
        
        if hasattr(self, 'llm') and hasattr(self.llm, 'current_provider'):
            # Remove provedor atual dos falhos para for√ßar re-teste
            if provider in self.llm.failed_providers:
                self.llm.failed_providers.remove(provider)
            
            # For√ßa re-inicializa√ß√£o
            self.llm.current_provider = None
            
            # Tenta inicializar com prioridade para o provedor solicitado
            original_priority = self.llm.provider_priority.copy()
            if provider in self.llm.provider_priority:
                self.llm.provider_priority = [provider] + [p for p in original_priority if p != provider]
            
            success = self.llm.initialize_best_provider()
            
            # Restaura prioridade original
            self.llm.provider_priority = original_priority
            
            return success and self.llm.current_provider == provider
        
        return False
    
    # CORRE√á√ÉO: Adiciona m√©todos √† classe
    PeninOmegaFusion.generate_text = generate_text
    PeninOmegaFusion.analyze_text = analyze_text
    PeninOmegaFusion.get_llm_status = get_llm_status
    PeninOmegaFusion.switch_provider = switch_provider

# CORRE√á√ÉO: Aplica integra√ß√£o
integrate_multi_api_methods()

if __name__ == "__main__":
    # Configura√ß√£o para melhor performance em CPU
    if HAS_TORCH:
        torch.set_num_threads(multiprocessing.cpu_count())
        if hasattr(torch, 'set_float32_matmul_precision'):
            torch.set_float32_matmul_precision('high')
    
    # Executa sistema
    asyncio.run(main())
