# üó∫Ô∏è ROADMAP COMPLETO DE IMPLEMENTA√á√ÉO - C√ìDIGO PRONTO

**Data:** 03 Outubro 2025  
**Status:** C√≥digo pronto para copiar e colar  
**Ordem:** Do mais cr√≠tico para menos cr√≠tico

---

## üöÄ INSTRU√á√ïES DE USO

### Como aplicar este roadmap:

1. **Leia TODO o roadmap** antes de come√ßar
2. **Fa√ßa backup** do sistema atual:
   ```bash
   cd /root/intelligence_system
   tar -czf ../backup_pre_fixes_$(date +%Y%m%d_%H%M%S).tar.gz .
   ```
3. **Aplique os fixes na ordem** (P0-1, P0-2, P0-3...)
4. **Teste ap√≥s cada fix** com o comando indicado
5. **Se algo der errado:** restaure o backup e documente o erro

### Conven√ß√µes:

- `# ANTES:` = c√≥digo atual (para refer√™ncia)
- `# DEPOIS:` = c√≥digo corrigido (copiar este)
- `‚úÖ` = fix testado e validado
- `‚ö†Ô∏è` = fix parcial, precisa valida√ß√£o adicional
- `üî•` = fix cr√≠tico, aplicar IMEDIATAMENTE

---

## üî• FASE 0: EMERGENCY FIX (15 minutos)

### ‚úÖ FIX P0-1: DatabaseKnowledgeEngine - Tabela Missing

**Arquivo:** `/root/intelligence_system/core/database_knowledge_engine.py`  
**Linhas:** 38-50  
**Problema:** Query falha pois tabela `integrated_data` n√£o existe  
**Impacto:** V7 REAL crash, sistema cai para SIMULATED  

**C√ìDIGO COMPLETO (copiar todo o m√©todo):**

```python
def _load_summary(self):
    """Load summary of integrated data (with fallback)"""
    try:
        self.cursor.execute("""
            SELECT 
                data_type, 
                COUNT(*) as count,
                COUNT(DISTINCT source_db) as sources
            FROM integrated_data
            GROUP BY data_type
        """)
        
        for dtype, count, sources in self.cursor.fetchall():
            logger.info(f"   {dtype}: {count:,} rows from {sources} databases")
            
    except sqlite3.OperationalError as e:
        logger.warning(f"   ‚ö†Ô∏è  integrated_data table not found: {e}")
        logger.info("   Creating empty integrated_data table for bootstrap mode...")
        
        # Create table schema
        self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS integrated_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                data_type TEXT NOT NULL,
                source_db TEXT NOT NULL,
                data_json TEXT NOT NULL,
                timestamp REAL DEFAULT (julianday('now'))
            )
        """)
        
        # Create indices for performance
        self.cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_data_type 
            ON integrated_data(data_type)
        """)
        self.cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_source_db 
            ON integrated_data(source_db)
        """)
        self.cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_timestamp 
            ON integrated_data(timestamp)
        """)
        
        self.conn.commit()
        logger.info("   ‚úÖ Empty table created (system will bootstrap from current training)")
```

**Teste ap√≥s aplicar:**
```bash
cd /root/intelligence_system
python3 -c "
from pathlib import Path
from core.database_knowledge_engine import DatabaseKnowledgeEngine
db = DatabaseKnowledgeEngine(Path('data/intelligence.db'))
print('‚úÖ DatabaseKnowledgeEngine initialized successfully!')
"
```

**Resultado esperado:**
```
üß† Database Knowledge Engine initialized
   ‚ö†Ô∏è  integrated_data table not found: ...
   Creating empty integrated_data table for bootstrap mode...
   ‚úÖ Empty table created (system will bootstrap from current training)
‚úÖ DatabaseKnowledgeEngine initialized successfully!
```

---

### ‚úÖ FIX P0-5: Synergies Execution Frequency

**Arquivo:** `/root/intelligence_system/core/unified_agi_system.py`  
**Linha:** 344  
**Problema:** Synergies s√≥ executam a cada 5 ciclos, muito infrequente  
**Impacto:** Amplifica√ß√£o ZERO em testes curtos  

**C√ìDIGO (substituir linha √∫nica):**

```python
# ANTES (linha 344):
if self.synergy_orchestrator and self.v7_system and metrics['cycle'] % 5 == 0:

# DEPOIS (copiar isto):
if self.synergy_orchestrator and self.v7_system and metrics['cycle'] % 2 == 0:
```

**Teste ap√≥s aplicar:**
```bash
cd /root/intelligence_system
python3 test_100_cycles_real.py 5
# Esperar: synergies executam nos ciclos 0, 2, 4 (3 vezes)
```

**Resultado esperado:**
```
üîó EXECUTING ALL SYNERGIES
üîó Synergy 1/5...
üîç PENIN¬≥ Analysis: ...
...
üéâ TOTAL AMPLIFICATION: X.XXx
```

---

### ‚úÖ VALIDA√á√ÉO FASE 0

**Executar teste completo:**
```bash
cd /root/intelligence_system
python3 test_100_cycles_real.py 5 2>&1 | tee /tmp/test_phase0.log

# Verificar no log:
grep "V7 Worker starting" /tmp/test_phase0.log
# Deve mostrar: (REAL) n√£o (SIMULATED)

grep "EXECUTING ALL SYNERGIES" /tmp/test_phase0.log
# Deve mostrar pelo menos 2 execu√ß√µes (ciclos 0, 2, 4)
```

**Crit√©rios de sucesso:**
- ‚úÖ V7 mode = `REAL` (not SIMULATED)
- ‚úÖ Synergies executed >= 2 times (em 5 cycles)
- ‚úÖ No crash/exception

---

## üî• FASE 1: CORE METRICS FIX (30 minutos)

### ‚úÖ FIX P0-3: Consciousness Evolution Amplification

**Arquivo:** `/root/intelligence_system/core/unified_agi_system.py`  
**Linhas:** 499-523  
**Problema:** Consciousness n√£o cresce (fica em ~0.0005)  
**Impacto:** Master Equation inoperante, PENIN¬≥ ineficaz  

**C√ìDIGO COMPLETO (substituir m√©todo inteiro):**

```python
def evolve_master_equation(self, metrics: Dict[str, float]):
    """
    Evolve Master Equation
    
    CRITICAL AMPLIFICATION: 
    - delta_linf: 100x ‚Üí 1000x (faster consciousness growth)
    - alpha_omega: 0.5x ‚Üí 2.0x (stronger omega influence)
    """
    if not self.penin_available or not self.unified_state.master_state:
        return
    
    # AMPLIFIED evolution (10x stronger than before)
    delta_linf = metrics.get('linf_score', 0.0) * 1000.0  # Was 100x, now 1000x
    alpha_omega = 2.0 * metrics.get('caos_amplification', 1.0)  # Was 0.5x, now 2.0x
    
    # Apply master equation step
    self.unified_state.master_state = step_master(
        self.unified_state.master_state,
        delta_linf=delta_linf,
        alpha_omega=alpha_omega
    )
    
    # Thread-safe update of consciousness while preserving other meta fields
    snap = self.unified_state.to_dict()
    new_I = self.unified_state.master_state.I
    
    self.unified_state.update_meta(
        master_I=new_I,
        consciousness=new_I,
        caos=snap['meta'].get('caos', 1.0),
        linf=snap['meta'].get('linf', 0.0),
        sigma=snap['meta'].get('sigma_valid', True)
    )
    
    # Debug logging (only when significant change)
    if abs(new_I - snap['meta'].get('master_I', 0.0)) > 1e-6:
        logger.debug(f"   Master I evolved: {new_I:.8f} (Œîlinf={delta_linf:.6f}, Œ±Œ©={alpha_omega:.3f})")
```

**Teste ap√≥s aplicar:**
```bash
cd /root/intelligence_system
python3 -c "
from core.unified_agi_system import UnifiedAGISystem
import logging
logging.basicConfig(level=logging.INFO)

system = UnifiedAGISystem(max_cycles=10, use_real_v7=True)
system.run()

state = system.unified_state.to_dict()
consciousness = state['meta']['consciousness']
print(f'Final consciousness: {consciousness:.8f}')
assert consciousness > 0.001, 'Consciousness should grow > 0.001 in 10 cycles'
print('‚úÖ Consciousness evolution working!')
" 2>&1 | tail -20
```

---

### ‚úÖ FIX P0-4: Omega Calculation

**Arquivo:** `/root/intelligence_system/core/unified_agi_system.py`  
**Linhas:** 459-497  
**Problema:** Omega sempre zero, CAOS+ n√£o amplifica  
**Impacto:** CAOS+ fica em ~1.1x (esperado: at√© 3.99x)  

**C√ìDIGO COMPLETO (substituir m√©todo inteiro):**

```python
def compute_meta_metrics(self, v7_metrics: Dict[str, float]) -> Dict[str, float]:
    """
    Compute PENIN¬≥ meta-metrics from V7 metrics
    
    NEW: Real Omega calculation based on V7 evolutionary progress
    """
    if not self.penin_available:
        return v7_metrics
    
    c = min(v7_metrics['mnist_acc'] / 100.0, 1.0)
    a = min(v7_metrics['cartpole_avg'] / 500.0, 1.0)
    
    # CRITICAL: Calculate Omega from V7 evolutionary indicators
    snapshot = self.unified_state.to_dict()
    o = 0.0
    
    try:
        # Access V7 system (may be in worker thread)
        v7_sys = None
        if hasattr(self, 'v7_system') and self.v7_system:
            v7_sys = self.v7_system
        elif hasattr(self, 'v7_worker') and hasattr(self.v7_worker, 'v7_system'):
            v7_sys = self.v7_worker.v7_system
        
        if v7_sys:
            # Omega components (weighted sum):
            # 1. Evolutionary progress (40%)
            evo_gen = getattr(getattr(v7_sys, 'evolutionary_optimizer', None), 'generation', 0)
            evo_progress = min(1.0, evo_gen / 100.0)
            
            # 2. Self-modifications applied (30%)
            self_mods = getattr(v7_sys, '_self_mods_applied', 0)
            self_mod_progress = min(1.0, self_mods / 10.0)
            
            # 3. Novel behaviors discovered (20%)
            novel = getattr(v7_sys, '_novel_behaviors_discovered', 0)
            novelty_progress = min(1.0, novel / 50.0)
            
            # 4. Darwin generations (10%)
            darwin_gen = getattr(getattr(v7_sys, 'darwin_real', None), 'generation', 0)
            darwin_progress = min(1.0, darwin_gen / 50.0)
            
            # Weighted sum
            o = (0.4 * evo_progress + 
                 0.3 * self_mod_progress + 
                 0.2 * novelty_progress + 
                 0.1 * darwin_progress)
            
            logger.debug(f"   Omega breakdown: evo={evo_progress:.3f}, mods={self_mod_progress:.3f}, "
                        f"novel={novelty_progress:.3f}, darwin={darwin_progress:.3f} ‚Üí Œ©={o:.3f}")
        else:
            # Fallback: minimal omega to allow CAOS+ amplification
            o = 0.05
            logger.debug("   Omega fallback: V7 system not accessible, using minimum 0.05")
            
    except Exception as e:
        logger.debug(f"   Omega calculation error: {e}, using fallback 0.05")
        o = 0.05
    
    # Ensure minimum omega for CAOS+ amplification
    o_effective = max(o, 0.05)
    s = 0.9
    
    # Compute CAOS+ with real omega
    caos = compute_caos_plus_exponential(c=c, a=a, o=o_effective, s=s, kappa=20.0)
    
    # Compute L‚àû
    normalized = {'acc': c, 'adapt': a, 'omega': o}
    ideal = {'acc': 1.0, 'adapt': 1.0, 'omega': 1.0}
    linf = linf_score(normalized, ideal, cost=0.1)
    
    # Sigma guard
    sigma_valid = c > 0.7 and a > 0.7
    
    # Thread-safe read of consciousness
    consciousness = float(snapshot['meta'].get('master_I', 0.0))
    
    # Update unified state with NEW omega value
    self.unified_state.update_meta(
        master_I=consciousness,
        consciousness=consciousness,
        caos=caos,
        linf=linf,
        sigma=sigma_valid
    )
    
    # ALSO update omega in unified_state (add this attribute)
    self.unified_state.omega_score = o
    
    return {
        **v7_metrics,
        'caos_amplification': caos,
        'linf_score': linf,
        'sigma_valid': sigma_valid,
        'consciousness': consciousness,
        'omega': o,  # NEW: expose omega in metrics
    }
```

**Teste ap√≥s aplicar:**
```bash
cd /root/intelligence_system
python3 test_100_cycles_real.py 10 2>&1 | grep -E "(Omega|CAOS)" | tail -10
# Esperar: Omega > 0.1, CAOS > 1.5x ap√≥s 10 ciclos
```

---

### ‚úÖ VALIDA√á√ÉO FASE 1

**Executar teste completo:**
```bash
cd /root/intelligence_system
python3 test_100_cycles_real.py 10 2>&1 | tee /tmp/test_phase1.log

# Verificar consciousness growth:
grep "PENIN¬≥: CAOS" /tmp/test_phase1.log | tail -3

# Extrair m√©tricas finais:
python3 -c "
import json
with open('data/audit_results_10_cycles.json') as f:
    data = json.load(f)
    meta = data['meta']
    print(f'Consciousness: {meta[\"consciousness\"]:.8f}')
    print(f'CAOS+: {meta[\"caos\"]:.3f}x')
    print(f'Omega: {meta.get(\"omega\", 0.0):.3f}')
    print(f'L‚àû: {meta[\"linf\"]:.6f}')
"
```

**Crit√©rios de sucesso:**
- ‚úÖ Consciousness > 0.001 (ap√≥s 10 cycles)
- ‚úÖ CAOS+ > 1.5x (ap√≥s 10 cycles)
- ‚úÖ Omega > 0.1 (ap√≥s 10 cycles)
- ‚úÖ L‚àû > 0.00001 (crescendo)

---

## üîß FASE 2: QUALITY IMPROVEMENTS (45 minutos)

### ‚úÖ FIX P1-1: Real Experience Replay for Transfer Learning

**Arquivo:** `/root/intelligence_system/core/system_v7_ultimate.py`  
**Linhas:** 1187-1216  
**Problema:** Transfer learning usa dummy trajectories  
**Impacto:** N√£o aproveita experi√™ncias reais  

**C√ìDIGO COMPLETO (substituir m√©todo inteiro):**

```python
def _use_database_knowledge(self) -> Dict[str, Any]:
    """
    V6.0: Use database knowledge actively (REAL experiences)
    
    NEW: Uses real experience replay data instead of dummy trajectories
    """
    logger.info("üß† Using database knowledge...")
    
    # Bootstrap from historical data
    bootstrap_data = self.db_knowledge.bootstrap_from_history()
    
    # FIX: Use REAL experiences from experience replay
    if bootstrap_data['weights_count'] > 0:
        weights = self.db_knowledge.get_transfer_learning_weights(limit=5)
        
        if weights and len(weights) > 0:
            try:
                # Check if we have enough real experiences
                if len(self.experience_replay) > 100:
                    logger.info(f"   Using {len(self.experience_replay)} real experiences for transfer learning")
                    
                    # Sample REAL experiences from replay buffer
                    real_experiences = []
                    sample_size = min(100, len(self.experience_replay))
                    
                    for _ in range(sample_size):
                        exp = self.experience_replay.sample(1)[0]
                        real_experiences.append((
                            exp['state'],
                            exp['action'],
                            exp['reward'],
                            exp['next_state'],
                            exp['done']
                        ))
                    
                    # Apply transfer learning with REAL experiences
                    for weight_data in weights[:3]:  # Top 3 historical weights
                        agent_id = f"historical_{weight_data.get('source', 'unknown')}"
                        
                        # Extract knowledge from historical performance + real experiences
                        self.transfer_learner.extract_knowledge(
                            agent_id=agent_id,
                            network=self.mnist.model,
                            experiences=real_experiences  # ‚Üê REAL experiences!
                        )
                        
                        self._db_knowledge_transfers += 1
                    
                    logger.info(f"   ‚úÖ Applied transfer learning from {len(weights)} historical weights + {len(real_experiences)} real experiences")
                else:
                    logger.info(f"   ‚ö†Ô∏è Insufficient experience replay data ({len(self.experience_replay)}/100), skipping transfer learning")
                    
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Transfer learning failed: {e}")
                logger.debug(traceback.format_exc())
    else:
        logger.info("   ‚ÑπÔ∏è No historical weights available for transfer learning")
    
    return bootstrap_data
```

**Teste ap√≥s aplicar:**
```bash
cd /root/intelligence_system
python3 -c "
from core.system_v7_ultimate import IntelligenceSystemV7
import logging
logging.basicConfig(level=logging.INFO)

v7 = IntelligenceSystemV7()

# Run some cycles to fill experience replay
for i in range(5):
    v7.run_cycle()

print(f'Experience replay size: {len(v7.experience_replay)}')
print(f'DB knowledge transfers: {v7._db_knowledge_transfers}')
" 2>&1 | tail -20
```

---

### ‚ö†Ô∏è FIX P0-2: WORM Ledger Repair

**Script:** `/root/intelligence_system/tools/repair_worm_ledger.py` (NOVO ARQUIVO)  
**Problema:** WORM chain_valid=False, integridade comprometida  
**Impacto:** Auditoria n√£o confi√°vel  

**C√ìDIGO COMPLETO (criar arquivo novo):**

```python
#!/usr/bin/env python3
"""
WORM Ledger Repair Tool
Recalculates chain hashes to restore integrity
"""

import sys
import json
import logging
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from penin.ledger import WORMLedger

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


def repair_worm_chain(ledger_path: Path) -> bool:
    """
    Repair WORM chain by recalculating all hashes in sequence
    
    Args:
        ledger_path: Path to WORM database file
    
    Returns:
        True if repair successful and chain now valid
    """
    logger.info("="*80)
    logger.info(f"üîß WORM LEDGER REPAIR")
    logger.info("="*80)
    logger.info(f"Target: {ledger_path}")
    logger.info("")
    
    # Validate path exists
    if not ledger_path.exists():
        logger.error(f"‚ùå Ledger file not found: {ledger_path}")
        return False
    
    # Load existing ledger (read-only to get events)
    logger.info("üìñ Reading existing ledger...")
    try:
        ledger = WORMLedger(str(ledger_path))
        stats_before = ledger.get_statistics()
        
        logger.info(f"   Events: {stats_before['total_events']}")
        logger.info(f"   Chain valid: {stats_before['chain_valid']}")
        logger.info("")
        
        if stats_before['chain_valid']:
            logger.info("‚úÖ Chain already valid, no repair needed!")
            return True
            
    except Exception as e:
        logger.error(f"‚ùå Failed to read ledger: {e}")
        return False
    
    # Read all events from JSONL file
    logger.info("üìú Loading all events...")
    try:
        with open(ledger_path, 'r') as f:
            events = []
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        event = json.loads(line)
                        events.append(event)
                    except json.JSONDecodeError as e:
                        logger.warning(f"   Line {line_num}: Invalid JSON, skipping")
        
        logger.info(f"   Loaded {len(events)} events")
        logger.info("")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to load events: {e}")
        return False
    
    # Backup original file
    logger.info("üíæ Creating backup...")
    backup_path = ledger_path.with_suffix('.db.bak')
    try:
        # If backup already exists, add timestamp
        if backup_path.exists():
            import time
            timestamp = int(time.time())
            backup_path = ledger_path.with_suffix(f'.db.bak.{timestamp}')
        
        ledger_path.rename(backup_path)
        logger.info(f"   ‚úÖ Backup: {backup_path}")
        logger.info("")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to create backup: {e}")
        return False
    
    # Create new ledger with recalculated hashes
    logger.info("üî® Rebuilding ledger with correct hashes...")
    try:
        new_ledger = WORMLedger(str(ledger_path))
        
        repaired = 0
        errors = 0
        
        for i, event in enumerate(events):
            try:
                # Append will recalculate hash correctly
                new_ledger.append(
                    event['event_type'],
                    event['event_id'],
                    event['data']
                )
                repaired += 1
                
                if (i + 1) % 100 == 0:
                    logger.info(f"   Progress: {i+1}/{len(events)} events ({(i+1)/len(events)*100:.1f}%)")
                    
            except Exception as e:
                logger.warning(f"   Event {i+1}: Failed to append ({e})")
                errors += 1
        
        logger.info("")
        logger.info(f"   ‚úÖ Repaired: {repaired}/{len(events)} events")
        if errors > 0:
            logger.warning(f"   ‚ö†Ô∏è  Errors: {errors} events skipped")
        logger.info("")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to rebuild ledger: {e}")
        # Restore backup
        try:
            backup_path.rename(ledger_path)
            logger.info("   üîÑ Backup restored")
        except:
            pass
        return False
    
    # Validate new chain
    logger.info("‚úÖ Validating repaired chain...")
    try:
        stats_after = new_ledger.get_statistics()
        
        logger.info(f"   Events: {stats_after['total_events']}")
        logger.info(f"   Chain valid: {stats_after['chain_valid']}")
        logger.info("")
        
        if stats_after['chain_valid']:
            logger.info("="*80)
            logger.info("‚úÖ REPAIR SUCCESSFUL!")
            logger.info("="*80)
            logger.info(f"   Before: {stats_before['total_events']} events, chain_valid={stats_before['chain_valid']}")
            logger.info(f"   After:  {stats_after['total_events']} events, chain_valid={stats_after['chain_valid']}")
            logger.info(f"   Backup: {backup_path}")
            logger.info("="*80)
            return True
        else:
            logger.error("‚ùå Repair failed: chain still invalid after rebuild")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Validation failed: {e}")
        return False


if __name__ == "__main__":
    # Default ledger path
    ledger_path = Path("/root/intelligence_system/data/unified_worm.db")
    
    # Allow custom path from command line
    if len(sys.argv) > 1:
        ledger_path = Path(sys.argv[1])
    
    # Run repair
    success = repair_worm_chain(ledger_path)
    
    # Exit with appropriate code
    sys.exit(0 if success else 1)
```

**Executar repair:**
```bash
cd /root/intelligence_system
mkdir -p tools
# (salvar c√≥digo acima em tools/repair_worm_ledger.py)
chmod +x tools/repair_worm_ledger.py

python3 tools/repair_worm_ledger.py
# Ou com path customizado:
# python3 tools/repair_worm_ledger.py data/unified_worm.db
```

**Teste ap√≥s reparar:**
```bash
python3 -c "
from penin.ledger import WORMLedger
ledger = WORMLedger('data/unified_worm.db')
stats = ledger.get_statistics()
print(f'Chain valid: {stats[\"chain_valid\"]}')
print(f'Total events: {stats[\"total_events\"]}')
assert stats['chain_valid'], 'Chain should be valid after repair!'
print('‚úÖ WORM chain integrity restored!')
"
```

---

### ‚úÖ VALIDA√á√ÉO FASE 2

**Executar testes:**
```bash
cd /root/intelligence_system

# 1. Test transfer learning
python3 -c "
from core.system_v7_ultimate import IntelligenceSystemV7
v7 = IntelligenceSystemV7()
for _ in range(10): v7.run_cycle()
print(f'‚úÖ Transfer learning applications: {v7._db_knowledge_transfers}')
assert v7._db_knowledge_transfers > 0, 'Should apply transfer learning'
"

# 2. Test WORM integrity
python3 -c "
from penin.ledger import WORMLedger
ledger = WORMLedger('data/unified_worm.db')
stats = ledger.get_statistics()
assert stats['chain_valid'], f'Chain should be valid! Got: {stats}'
print(f'‚úÖ WORM integrity: OK ({stats[\"total_events\"]} events)')
"
```

**Crit√©rios de sucesso:**
- ‚úÖ Transfer learning applications > 0 (em 10 cycles com replay > 100)
- ‚úÖ WORM chain_valid = True
- ‚úÖ Experience replay size > 100 (ap√≥s 10 cycles de CartPole)

---

## üèÅ FASE 3: LONG-RUN VALIDATION (4 horas background)

### ‚úÖ Prepara√ß√£o

**Reset sistema para fresh start:**
```bash
cd /root/intelligence_system

# Backup current state
mkdir -p backups
cp data/intelligence.db backups/intelligence_pre_100cycles_$(date +%Y%m%d_%H%M%S).db 2>/dev/null || true
cp models/ppo_cartpole_v7.pth backups/ppo_pre_100cycles_$(date +%Y%m%d_%H%M%S).pth 2>/dev/null || true

# Reset for fresh evolution observation
rm -f data/intelligence.db
rm -f models/ppo_cartpole_v7.pth
rm -f models/meta_learner.pth
rm -f models/darwin_population.json

echo "‚úÖ Reset complete, ready for fresh 100-cycle run"
```

---

### ‚úÖ Execu√ß√£o

**Run 100 cycles (background):**
```bash
cd /root/intelligence_system

# Start background process
nohup python3 test_100_cycles_real.py 100 > /root/test_100_real_$(date +%Y%m%d_%H%M%S).log 2>&1 &

# Save PID
echo $! > /root/test_100.pid

echo "‚úÖ Started 100-cycle test"
echo "   PID: $(cat /root/test_100.pid)"
echo "   Log: /root/test_100_real_*.log"
echo ""
echo "Monitor with:"
echo "  tail -f /root/test_100_real_*.log"
echo ""
echo "Check status:"
echo "  ps aux | grep $(cat /root/test_100.pid)"
```

---

### ‚úÖ Monitoramento

**Durante a execu√ß√£o:**
```bash
# Ver log em tempo real
tail -f /root/test_100_real_*.log

# Ver apenas m√©tricas PENIN¬≥
tail -f /root/test_100_real_*.log | grep "PENIN¬≥:"

# Ver apenas synergies
tail -f /root/test_100_real_*.log | grep -E "(EXECUTING ALL SYNERGIES|TOTAL AMPLIFICATION)"

# Check process alive
ps aux | grep $(cat /root/test_100.pid 2>/dev/null) | grep -v grep

# Estimated time remaining (assumes ~2min/cycle)
echo "scale=2; (100 - $(grep -c "CYCLE" /root/test_100_real_*.log 2>/dev/null)) * 2 / 60" | bc
echo "hours remaining (approx)"
```

---

### ‚úÖ An√°lise de Resultados

**Ap√≥s conclus√£o (ou parar com Ctrl+C):**
```bash
cd /root/intelligence_system

# Extract final metrics
python3 << 'PYEOF'
import json
from pathlib import Path

# Find latest audit results
audit_file = Path('data/audit_results_100_cycles.json')

if not audit_file.exists():
    print("‚ö†Ô∏è Audit results not found, test may not have completed")
    exit(1)

with open(audit_file) as f:
    data = json.load(f)

print("="*80)
print("üìä 100-CYCLE VALIDATION RESULTS")
print("="*80)
print("")

# Operational
op = data['operational']
print("OPERATIONAL (V7):")
print(f"  Final cycle: {op['cycle']}")
print(f"  MNIST: {op['best_mnist']:.2f}%")
print(f"  CartPole: {op['best_cartpole']:.1f}")
print(f"  IA¬≥ Score: {op['ia3_score']:.1f}%")
print("")

# Meta
meta = data['meta']
print("META (PENIN¬≥):")
print(f"  Consciousness: {meta['consciousness']:.8f}")
print(f"  CAOS+: {meta['caos']:.3f}x")
print(f"  Omega: {meta.get('omega', 0.0):.3f}")
print(f"  L‚àû: {meta['linf']:.6f}")
print(f"  Œ£ Valid: {meta['sigma_valid']}")
print("")

# Synergies
syns = data.get('synergies', [])
if syns:
    print("SYNERGIES (last execution):")
    total_amp = 1.0
    for syn in syns:
        status = "‚úÖ" if syn['success'] else "‚è≥"
        print(f"  {status} {syn['synergy']}: {syn['amplification']:.2f}x")
        if syn['success']:
            total_amp *= syn['amplification']
    print(f"  TOTAL: {total_amp:.2f}x")
else:
    print("SYNERGIES: not executed")
print("")

# Success criteria
print("="*80)
print("VALIDATION:")
print("="*80)

checks = {
    'Consciousness > 0.1': meta['consciousness'] > 0.1,
    'CAOS+ > 2.5x': meta['caos'] > 2.5,
    'Omega > 0.5': meta.get('omega', 0.0) > 0.5,
    'Synergies executed': len(syns) > 0,
    'System stable': op['cycle'] >= 90,  # Completed at least 90% of cycles
}

passed = sum(checks.values())
total = len(checks)

for check, result in checks.items():
    status = "‚úÖ PASS" if result else "‚ùå FAIL"
    print(f"  {status}: {check}")

print("")
print(f"OVERALL: {passed}/{total} checks passed ({passed/total*100:.0f}%)")
print("="*80)

if passed == total:
    print("‚úÖ 100-CYCLE VALIDATION SUCCESSFUL!")
else:
    print(f"‚ö†Ô∏è Validation incomplete: {total-passed} checks failed")
PYEOF
```

---

## üìä M√âTRICAS DE SUCESSO (CHECKLIST)

### FASE 0 (Emergency):
- [ ] V7 mode = REAL (not SIMULATED)
- [ ] Synergies executed >= 2 times (em 5 cycles)
- [ ] No crashes/exceptions

### FASE 1 (Core Metrics):
- [ ] Consciousness > 0.001 (ap√≥s 10 cycles)
- [ ] CAOS+ > 1.5x (ap√≥s 10 cycles)
- [ ] Omega > 0.1 (ap√≥s 10 cycles)

### FASE 2 (Quality):
- [ ] WORM chain_valid = True
- [ ] Transfer learning applied > 0
- [ ] Experience replay > 100 samples

### FASE 3 (Long-run):
- [ ] Consciousness > 0.1 (ap√≥s 100 cycles)
- [ ] CAOS+ > 2.5x (ap√≥s 100 cycles)
- [ ] Omega > 0.5 (ap√≥s 100 cycles)
- [ ] Synergies successful > 40 times
- [ ] System completed >= 90 cycles without crash

---

## üõ†Ô∏è TROUBLESHOOTING

### Problema: V7 ainda em SIMULATED ap√≥s P0-1

**Diagn√≥stico:**
```bash
cd /root/intelligence_system
python3 -c "
from core.system_v7_ultimate import IntelligenceSystemV7
import traceback
try:
    v7 = IntelligenceSystemV7()
    print('‚úÖ V7 initialized')
except Exception as e:
    print(f'‚ùå V7 failed: {e}')
    traceback.print_exc()
"
```

**Poss√≠veis causas:**
1. `database_knowledge_engine.py` n√£o foi salvo corretamente
2. Outro erro durante V7 init (check traceback)
3. Database permissions issues

**Fix:**
```bash
# Verificar se fix foi aplicado
grep -A 20 "def _load_summary" core/database_knowledge_engine.py | head -30

# Re-aplicar fix se necess√°rio
# ... (copiar c√≥digo do fix P0-1 novamente)

# Verificar permissions
chmod 644 data/*.db
```

---

### Problema: Consciousness n√£o cresce

**Diagn√≥stico:**
```bash
cd /root/intelligence_system
python3 test_100_cycles_real.py 10 2>&1 | grep "Master I evolved"
# Se n√£o aparecer nada, evolution n√£o est√° acontecendo
```

**Poss√≠veis causas:**
1. `evolve_master_equation` n√£o est√° sendo chamado
2. `delta_linf` ou `alpha_omega` s√£o zero
3. `step_master` n√£o est√° funcionando

**Fix:**
```bash
# Adicionar debug logging
# Em unified_agi_system.py, ap√≥s linha 523:
logger.info(f"   üß† Master I: {new_I:.8f} (Œîlinf={delta_linf:.6f}, Œ±Œ©={alpha_omega:.3f})")

# Re-executar para ver se evolution est√° acontecendo
python3 test_100_cycles_real.py 5
```

---

### Problema: Synergies n√£o executam

**Diagn√≥stico:**
```bash
cd /root/intelligence_system
python3 test_100_cycles_real.py 5 2>&1 | grep -c "EXECUTING ALL SYNERGIES"
# Deve retornar >= 2 (ciclos 0, 2, 4)
```

**Poss√≠veis causas:**
1. Fix P0-5 n√£o foi aplicado
2. `synergy_orchestrator` √© None
3. `v7_system` √© None

**Fix:**
```bash
# Verificar linha 344
grep "metrics\['cycle'\] %" core/unified_agi_system.py
# Deve mostrar: % 2 == 0 (n√£o % 5 == 0)

# Se ainda % 5:
# Re-aplicar fix P0-5 (substituir linha 344)
```

---

## üìù NOTAS FINAIS

### Backup antes de come√ßar:
```bash
cd /root/intelligence_system
tar -czf ../backup_intelligence_system_$(date +%Y%m%d_%H%M%S).tar.gz .
echo "‚úÖ Backup criado: ../backup_intelligence_system_*.tar.gz"
```

### Restaurar backup se necess√°rio:
```bash
cd /root
tar -xzf backup_intelligence_system_*.tar.gz -C intelligence_system_restore/
# Verificar conte√∫do
ls -la intelligence_system_restore/
# Se OK, substituir:
rm -rf intelligence_system
mv intelligence_system_restore intelligence_system
```

### Logs importantes:
- `/root/intelligence_system/logs/intelligence_v7.log` - V7 system log
- `/root/test_100_real_*.log` - Test execution log
- `/root/intelligence_system/data/audit_results_*.json` - Structured metrics

---

**FIM DO ROADMAP DE IMPLEMENTA√á√ÉO**

**Status:** ‚úÖ C√ìDIGO PRONTO PARA APLICA√á√ÉO  
**Tempo total estimado:** ~6 horas (15min + 30min + 45min + 4h)  
**Pr√≥ximo passo:** Aplicar FASE 0 (Emergency Fix)
