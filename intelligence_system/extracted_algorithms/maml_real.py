"""
MAML REAL - Few-shot learning funcional
"""
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import logging
from copy import deepcopy

logger = logging.getLogger(__name__)


class MAMLReal:
    """MAML simplificado mas FUNCIONAL"""
    
    def __init__(self, input_size: int, output_size: int, hidden_size: int = 64,
                 inner_lr: float = 0.01, outer_lr: float = 0.001):
        
        self.input_size = input_size
        self.output_size = output_size
        self.inner_lr = inner_lr
        self.outer_lr = outer_lr
        
        # Base model
        self.model = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size)
        )
        
        # Meta-optimizer
        self.meta_optimizer = optim.Adam(self.model.parameters(), lr=outer_lr)
        self.criterion = nn.CrossEntropyLoss()
        
        logger.info(f"‚úÖ MAML Real initialized")
    
    def inner_loop(self, support_x, support_y, n_steps: int = 5):
        """
        Inner loop: Adapt to task with few examples
        
        Returns adapted model
        """
        # Clone model for adaptation (preserve gradients)
        adapted_model = deepcopy(self.model)
        
        # Set requires_grad for adaptation
        for param in adapted_model.parameters():
            param.requires_grad = True
        
        inner_optimizer = optim.SGD(adapted_model.parameters(), lr=self.inner_lr)
        
        # Adapt for n_steps
        for step in range(n_steps):
            output = adapted_model(support_x)
            loss = self.criterion(output, support_y)
            
            inner_optimizer.zero_grad()
            loss.backward()
            inner_optimizer.step()
        
        return adapted_model
    
    def meta_train_step(self, support_x, support_y, query_x, query_y, n_inner_steps: int = 5):
        """
        Meta-training step: Train to adapt quickly
        
        Returns query loss after adaptation
        """
        # Inner loop (adapt to support)
        adapted_model = self.inner_loop(support_x, support_y, n_inner_steps)
        
        # Outer loop (evaluate on query)
        query_output = adapted_model(query_x)
        query_loss = self.criterion(query_output, query_y)
        
        # Meta-update (update base model)
        self.meta_optimizer.zero_grad()
        query_loss.backward()
        self.meta_optimizer.step()
        
        # Compute accuracy
        with torch.no_grad():
            query_acc = (query_output.argmax(1) == query_y).float().mean()
        
        return {
            'query_loss': query_loss.item(),
            'query_accuracy': query_acc.item() * 100
        }
    
    def fast_adapt_eval(self, support_x, support_y, query_x, query_y, n_inner_steps: int = 5):
        """
        Fast adaptation (evaluation mode - no meta-update)
        
        Returns performance on query set
        """
        # Create fresh adapted model matching base model architecture
        adapted_model = nn.Sequential(
            nn.Linear(self.input_size, self.model[0].out_features),  # Match hidden_size
            nn.ReLU(),
            nn.Linear(self.model[0].out_features, self.model[2].out_features),  # Match hidden_size
            nn.ReLU(),
            nn.Linear(self.model[2].out_features, self.output_size)
        )
        
        # Copy weights from base model
        adapted_model.load_state_dict(self.model.state_dict())
        
        # Adapt on support set
        optimizer = optim.SGD(adapted_model.parameters(), lr=self.inner_lr)
        
        for step in range(n_inner_steps):
            output = adapted_model(support_x)
            loss = self.criterion(output, support_y)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        # Evaluate on query
        with torch.no_grad():
            query_output = adapted_model(query_x)
            query_loss = self.criterion(query_output, query_y)
            query_acc = (query_output.argmax(1) == query_y).float().mean()
        
        return {
            'query_loss': query_loss.item(),
            'query_accuracy': query_acc.item() * 100
        }


def test_maml_real():
    """Test MAML REAL"""
    print("="*80)
    print("üî• MAML REAL - FEW-SHOT LEARNING")
    print("="*80)
    
    maml = MAMLReal(input_size=10, output_size=2, hidden_size=64)
    
    # Create simple binary classification task
    print("\nüìö Few-shot task (5-shot):")
    support_x = torch.randn(5, 10)
    support_y = torch.randint(0, 2, (5,))
    query_x = torch.randn(20, 10)
    query_y = torch.randint(0, 2, (20,))
    
    print(f"   Support: {support_x.shape}, {support_y.shape}")
    print(f"   Query: {query_x.shape}, {query_y.shape}")
    
    # Test 1: Fast adaptation (before meta-training)
    print("\n1Ô∏è‚É£ Before meta-training:")
    result_before = maml.fast_adapt_eval(support_x, support_y, query_x, query_y, n_inner_steps=5)
    print(f"   Loss: {result_before['query_loss']:.3f}")
    print(f"   Accuracy: {result_before['query_accuracy']:.1f}%")
    
    # Test 2: Meta-train on multiple tasks
    print("\n2Ô∏è‚É£ Meta-training (50 tasks)...")
    
    for task_i in range(50):
        # Generate random task
        s_x = torch.randn(5, 10)
        s_y = torch.randint(0, 2, (5,))
        q_x = torch.randn(20, 10)
        q_y = torch.randint(0, 2, (20,))
        
        result = maml.meta_train_step(s_x, s_y, q_x, q_y, n_inner_steps=5)
        
        if (task_i + 1) % 25 == 0:
            print(f"   Task {task_i+1}: loss={result['query_loss']:.3f}, acc={result['query_accuracy']:.1f}%")
    
    # Test 3: Fast adaptation (after meta-training)
    print("\n3Ô∏è‚É£ After meta-training:")
    result_after = maml.fast_adapt_eval(support_x, support_y, query_x, query_y, n_inner_steps=5)
    print(f"   Loss: {result_after['query_loss']:.3f}")
    print(f"   Accuracy: {result_after['query_accuracy']:.1f}%")
    
    # Compare
    print(f"\nüìä Compara√ß√£o:")
    print(f"   Loss: {result_before['query_loss']:.3f} ‚Üí {result_after['query_loss']:.3f}")
    print(f"   Accuracy: {result_before['query_accuracy']:.1f}% ‚Üí {result_after['query_accuracy']:.1f}%")
    
    if result_after['query_loss'] < result_before['query_loss']:
        print(f"\n‚úÖ MAML FUNCIONA!")
        print(f"   Meta-learning REAL")
        print(f"   Few-shot adaptation melhora com meta-training")
    else:
        print(f"\n‚ö†Ô∏è N√£o melhorou significativamente")
    
    return True


if __name__ == "__main__":
    test_maml_real()
