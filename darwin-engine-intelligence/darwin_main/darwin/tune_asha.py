"""
HPO com Ray Tune + ASHA: explora lr, batch e pol√≠tica (X_PER_DEATH_SPAWN).
Executa ciclos curtos e seleciona configs promissoras.

Refer√™ncia: Ray Tune + ASHA Scheduler
https://docs.ray.io/en/latest/tune/api/schedulers.html#asha-scheduler
"""
import os, random, time
import torch, torch.nn as nn

try:
    from ray import tune
    from ray.tune.schedulers import ASHAScheduler
    RAY_AVAILABLE = True
except ImportError:
    RAY_AVAILABLE = False

from .data_sources import SyntheticTask
from .population import Population

async def objective(config):
    """
    Objetivo para otimiza√ß√£o ASHA:
    Executa Darwin com configura√ß√£o espec√≠fica e retorna m√©tricas
    """
    if not RAY_AVAILABLE:
        raise RuntimeError("Ray Tune n√£o dispon√≠vel. Instale com: pip install 'ray[tune]'")
    
    # Setup determin√≠stico
    random.seed(config["seed"])
    torch.manual_seed(config["seed"])
    
    # Task e popula√ß√£o conforme config
    task = SyntheticTask(d_in=8, batch=config["batch"])
    loss_fn = nn.MSELoss()
    
    pop = Population(in_dim=8, out_dim=1, seed=config["seed"])
    pop.X_PER_DEATH_SPAWN = config["x_per_death_spawn"]
    
    # Ajustar learning rates da popula√ß√£o
    for neuron in pop.neurons:
        for group in neuron.opt.param_groups:
            group["lr"] = config["lr"]
        for group in neuron.opt_arch.param_groups:
            group["lr"] = config["arch_lr"]
    
    # Executar ciclos curtos para avalia√ß√£o
    losses = []
    consciousness_scores = []
    survival_rates = []
    
    budget_cycles = config.get("budget_cycles", 20)  # Or√ßamento curto para ASHA
    
    for cycle in range(budget_cycles):
        # Gerar dados
        x, y = task.batch()
        
        # Treino da popula√ß√£o
        train_loss = pop.train_step(x, y, loss_fn)
        losses.append(train_loss)
        
        # Avalia√ß√£o IA¬≥ e sele√ß√£o natural
        pop.evaluate_and_cull(train_loss)
        
        # M√©tricas para ASHA
        consciousness_scores.append(pop.collective_consciousness)
        
        # Taxa de sobreviv√™ncia (neur√¥nios que passaram na avalia√ß√£o IA¬≥)
        if pop.neurons:
            passed = sum(1 for n in pop.neurons if n.stats.get("ia3_score", 0) >= 0.6)
            survival_rate = passed / len(pop.neurons)
        else:
            survival_rate = 0.0
        survival_rates.append(survival_rate)
        
        # Nascimento obrigat√≥rio se popula√ß√£o muito pequena
        if len(pop.neurons) < 2:
            pop.add_neuron()
        
        # Reportar progresso intermedi√°rio para ASHA
        if cycle % 5 == 0:
            tune.report(
                intermediate_loss=train_loss,
                intermediate_consciousness=pop.collective_consciousness,
                intermediate_survival_rate=survival_rate,
                population_size=len(pop.neurons)
            )
    
    # M√©tricas finais
    final_loss = sum(losses[-5:]) / 5 if len(losses) >= 5 else (losses[-1] if losses else float("inf"))
    final_consciousness = sum(consciousness_scores[-5:]) / 5 if len(consciousness_scores) >= 5 else 0.0
    final_survival_rate = sum(survival_rates[-5:]) / 5 if len(survival_rates) >= 5 else 0.0
    
    # Fun√ß√£o objetivo composta (minimizar)
    # Menor loss + maior consci√™ncia + maior taxa de sobreviv√™ncia
    objective_score = final_loss - 0.5 * final_consciousness - 0.3 * final_survival_rate
    
    # Reportar resultado final
    tune.report(
        final_loss=final_loss,
        final_consciousness=final_consciousness,
        final_survival_rate=final_survival_rate,
        objective_score=objective_score,
        final_neurons=len(pop.neurons),
        total_births=pop.births,
        total_deaths=pop.deaths
    )

async def run_asha_optimization():
    """
    Executa otimiza√ß√£o ASHA para encontrar melhores hiperpar√¢metros Darwin
    """
    if not RAY_AVAILABLE:
        logger.info("‚ö†Ô∏è Ray Tune n√£o dispon√≠vel. Instale com: pip install 'ray[tune]'")
        return await None
    
    logger.info("üîç Iniciando otimiza√ß√£o HPO com Ray Tune + ASHA...")
    
    # Espa√ßo de busca
    search_space = {
        "seed": 42,  # Fixo para reprodutibilidade
        "batch": tune.choice([32, 64, 128]),
        "lr": tune.loguniform(1e-4, 1e-2),
        "arch_lr": tune.loguniform(1e-5, 1e-3),
        "x_per_death_spawn": tune.choice([5, 10, 15, 20]),
        "budget_cycles": tune.choice([15, 20, 25])
    }
    
    # Scheduler ASHA (early stopping agressivo)
    scheduler = ASHAScheduler(
        metric="objective_score",
        mode="min",
        max_t=25,  # M√°ximo de ciclos por trial
        grace_period=5,  # M√≠nimo antes de parar
        reduction_factor=2  # Agressividade do corte
    )
    
    # Configura√ß√£o do Tuner
    tuner = tune.Tuner(
        objective,
        tune_config=tune.TuneConfig(
            metric="objective_score",
            mode="min",
            scheduler=scheduler,
            num_samples=20  # N√∫mero de configura√ß√µes a testar
        ),
        run_config=tune.RunConfig(
            name="darwin_ia3_asha",
            local_dir="/root/darwin/logs/ray_results"
        ),
        param_space=search_space,
    )
    
    # Executar otimiza√ß√£o
    logger.info(f"   üéØ Testando {search_space} configura√ß√µes...")
    logger.info(f"   ‚ö° ASHA: early stopping agressivo ativo")
    
    results = tuner.fit()
    
    # Melhor resultado
    best_result = results.get_best_result()
    best_config = best_result.config
    best_metrics = best_result.metrics
    
    logger.info(f"\nüèÜ MELHOR CONFIGURA√á√ÉO ENCONTRADA:")
    logger.info(f"   LR: {best_config['lr']:.6f}")
    logger.info(f"   Arch LR: {best_config['arch_lr']:.6f}")
    logger.info(f"   Batch: {best_config['batch']}")
    logger.info(f"   Mortes por nascimento: {best_config['x_per_death_spawn']}")
    logger.info(f"   Loss final: {best_metrics['final_loss']:.6f}")
    logger.info(f"   Consci√™ncia final: {best_metrics['final_consciousness']:.3f}")
    logger.info(f"   Taxa sobreviv√™ncia: {best_metrics['final_survival_rate']:.3f}")
    
    # Salvar melhor configura√ß√£o
    config_path = "/root/darwin/logs/best_config.json"
    os.makedirs(os.path.dirname(config_path), exist_ok=True)
    
    with open(config_path, "w") as f:
        import json
        json.dump({
            "best_config": best_config,
            "best_metrics": best_metrics,
            "optimization_time": time.time(),
            "total_trials": len(results)
        }, f, indent=2)
    
    logger.info(f"   üíæ Configura√ß√£o salva em: {config_path}")
    
    return await best_config, best_metrics

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--optimize":
        # Executar otimiza√ß√£o ASHA
        if RAY_AVAILABLE:
            run_asha_optimization()
        else:
            logger.info("‚ùå Ray Tune n√£o dispon√≠vel")
    else:
        logger.info("Uso:")
        logger.info("  python tune_asha.py --optimize    # Executar otimiza√ß√£o ASHA")
        logger.info("")
        logger.info("Requires: pip install 'ray[tune]'")
        logger.info("Docs: https://docs.ray.io/en/latest/tune/api/schedulers.html#asha-scheduler")