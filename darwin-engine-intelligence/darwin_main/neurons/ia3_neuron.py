# /root/darwin/neurons/ia3_neuron.py
# CÃ©lula-neurÃ´nio IAÂ³ real (PyTorch): auto-mutaÃ§Ã£o, auto-evoluÃ§Ã£o, prova de vida

import math, random, json, os, time
from dataclasses import dataclass
from typing import Dict, Tuple, Any, Optional
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from datetime import datetime

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURAÃ‡ÃƒO E CONSTANTES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ACTIVATIONS = {
    "relu": nn.ReLU,
    "tanh": nn.Tanh,
    "gelu": nn.GELU,
    "silu": nn.SiLU,
}

@dataclass
class TrainConfig:
    input_dim: int = 16
    hidden_dim: int = 16
    lr: float = 1e-2
    steps: int = 200
    seed: int = 42
    batch: int = 64
    device: str = "cpu"
    act: str = "relu"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GERAÃ‡ÃƒO DE DADOS SINTÃ‰TICOS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def _make_data(n=512, d_in=16, seed=0, complexity_level=1):
    """
    Gera dados sintÃ©ticos com diferentes nÃ­veis de complexidade
    para testar capacidades adaptativas do neurÃ´nio
    """
    g = torch.Generator().manual_seed(seed)
    X = torch.randn(n, d_in, generator=g)
    
    if complexity_level == 1:
        # NÃ­vel 1: FunÃ§Ã£o linear com ruÃ­do
        weights = torch.randn(d_in, generator=g) * 0.1
        y = (X * weights).sum(dim=1, keepdim=True)
        y += 0.05 * torch.randn(n, 1, generator=g)
    
    elif complexity_level == 2:
        # NÃ­vel 2: FunÃ§Ã£o nÃ£o-linear suave
        y = (X[:, :4].sum(dim=1) + 0.5*torch.sin(X[:, 4:8]).sum(dim=1)).unsqueeze(1)
        y = (y - y.mean()) / (y.std() + 1e-6)
    
    else:
        # NÃ­vel 3: FunÃ§Ã£o complexa multimodal
        y1 = torch.sum(X[:, :4], dim=1)
        y2 = torch.prod(X[:, 4:8].abs() + 1e-6, dim=1).log()
        y3 = torch.norm(X[:, 8:12], dim=1)
        y = torch.stack([y1, y2, y3], dim=1).mean(dim=1, keepdim=True)
        y = (y - y.mean()) / (y.std() + 1e-6)
    
    # Split treino/validaÃ§Ã£o
    n_train = int(0.8 * n)
    return await (X[:n_train], y[:n_train]), (X[n_train:], y[n_train:])

async def _make_ood_data(base_X, base_y, shift_factor=0.2):
    """Cria dados Out-of-Distribution para teste de adaptaÃ§Ã£o"""
    # Shift na distribuiÃ§Ã£o
    ood_X = base_X + shift_factor * torch.randn_like(base_X)
    
    # RotaÃ§Ã£o leve
    if base_X.size(1) >= 2:
        angle = shift_factor * 0.1
        cos_a, sin_a = math.cos(angle), math.sin(angle)
        x0, x1 = ood_X[:, 0].clone(), ood_X[:, 1].clone()
        ood_X[:, 0] = cos_a * x0 - sin_a * x1
        ood_X[:, 1] = sin_a * x0 + cos_a * x1
    
    # Target levemente ajustado
    ood_y = base_y + shift_factor * 0.1 * torch.randn_like(base_y)
    
    return await ood_X, ood_y

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CÃ‰LULA-NEURÃ”NIO IAÂ³ AVANÃ‡ADA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class IA3Neuron(nn.Module):
    """
    Um 'neurÃ´nio-cÃ©lula' IAÂ³-like: mÃ³dulo mÃ­nimo capaz de:
    - aprender (backprop)
    - se auto-mutacionar (trocar ativaÃ§Ã£o / inserir gate / perturbar pesos)
    - provar melhora em validaÃ§Ã£o (delta_loss<0)
    - produzir 'novelty' (mudanÃ§a paramÃ©trica/estrutural)
    - calcular prÃ³pria consciÃªncia
    """
    async def __init__(self, cfg: TrainConfig):
        super().__init__()
        torch.manual_seed(cfg.seed)
        random.seed(cfg.seed)
        
        self.cfg = cfg
        self.neuron_id = f"n{random.randint(10_000, 99_999)}"
        self.birth_time = datetime.utcnow()
        self.age = 0
        self.generation = 0
        
        # Arquitetura inicial: neurÃ´nio simples
        self.fc = nn.Linear(cfg.input_dim, 1)
        self.gate = nn.Identity()  # Pode evoluir para nn.Linear(1,1)
        self.act = ACTIVATIONS.get(cfg.act, nn.ReLU)()
        
        # Estado evolutivo
        self.evolution_history = []
        self.mutation_count = 0
        self.successful_mutations = 0
        self.consciousness_score = 0.0
        
        # Snapshot dos pesos para calcular novidade
        self.register_buffer("last_weight_snapshot", torch.zeros_like(self.fc.weight))
        self.last_weight_snapshot.copy_(self.fc.weight.detach())
        
        # InicializaÃ§Ã£o cuidadosa
        nn.init.kaiming_uniform_(self.fc.weight, a=math.sqrt(5))
        nn.init.zeros_(self.fc.bias)

    async def forward(self, x):
        """Forward pass: fc â†’ gate â†’ ativaÃ§Ã£o"""
        z = self.fc(x)
        z = self.gate(z)
        return await self.act(z)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # AUTO-MUTAÃ‡ÃƒO E AUTO-EVOLUÃ‡ÃƒO
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def propose_self_mutation(self) -> Dict[str, Any]:
        """
        Auto-evoluÃ§Ã£o: neurÃ´nio propÃµe mudanÃ§as em si mesmo
        - Trocar ativaÃ§Ã£o
        - Evoluir gate (Identity â†’ Linear learnable)
        - PerturbaÃ§Ã£o de pesos direcionada
        """
        mutations_applied = []
        
        # MutaÃ§Ã£o 1: Troca de ativaÃ§Ã£o (25% chance)
        if random.random() < 0.25:
            current_act = self.cfg.act
            available_acts = [a for a in ACTIVATIONS.keys() if a != current_act]
            if available_acts:
                new_act = random.choice(available_acts)
                self.act = ACTIVATIONS[new_act]()
                self.cfg.act = new_act
                mutations_applied.append(f"activation_change_{current_act}_to_{new_act}")
        
        # MutaÃ§Ã£o 2: EvoluÃ§Ã£o do gate (15% chance)
        if random.random() < 0.15 and isinstance(self.gate, nn.Identity):
            self.gate = nn.Linear(1, 1, bias=False)
            nn.init.constant_(self.gate.weight, 1.0)  # Iniciar como identidade
            mutations_applied.append("gate_evolution_identity_to_linear")
        
        # MutaÃ§Ã£o 3: PerturbaÃ§Ã£o direcionada de pesos (30% chance)
        if random.random() < 0.30:
            with torch.no_grad():
                # PerturbaÃ§Ã£o pequena e direcionada
                noise_scale = 0.01 * (1.0 + self.successful_mutations * 0.1)  # Aumenta com sucesso
                noise = torch.randn_like(self.fc.weight) * noise_scale
                self.fc.weight.data += noise
                mutations_applied.append(f"weight_perturbation_scale_{noise_scale:.4f}")
        
        # MutaÃ§Ã£o 4: Ajuste de bias (10% chance)
        if random.random() < 0.10:
            with torch.no_grad():
                bias_adjustment = (random.random() - 0.5) * 0.05
                self.fc.bias.data += bias_adjustment
                mutations_applied.append(f"bias_adjustment_{bias_adjustment:.4f}")
        
        self.mutation_count += len(mutations_applied)
        
        return await {
            "mutations_applied": mutations_applied,
            "total_mutations": len(mutations_applied),
            "cumulative_mutations": self.mutation_count
        }

    async def measure_novelty(self) -> float:
        """
        Mede novidade estrutural/paramÃ©trica vs snapshot anterior
        """
        with torch.no_grad():
            # DistÃ¢ncia L2 dos pesos
            weight_distance = torch.norm(
                self.fc.weight - self.last_weight_snapshot, p=2
            ).item()
            
            # Normalizar pela magnitude dos pesos
            weight_magnitude = torch.norm(self.fc.weight, p=2).item()
            normalized_distance = weight_distance / (weight_magnitude + 1e-8)
            
            return await normalized_distance

    async def update_weight_snapshot(self):
        """Atualiza snapshot dos pesos para prÃ³xima mediÃ§Ã£o de novidade"""
        self.last_weight_snapshot.copy_(self.fc.weight.detach())

    async def calculate_consciousness(self, performance_metrics: Dict[str, Any]) -> float:
        """
        Calcula score de consciÃªncia baseado em:
        - Complexidade arquitetural
        - Capacidade adaptativa
        - MemÃ³ria temporal (idade/experiÃªncia)
        - Auto-evoluÃ§Ã£o bem-sucedida
        """
        # Complexidade arquitetural
        has_learnable_gate = isinstance(self.gate, nn.Linear)
        arch_complexity = 0.5 + (0.3 if has_learnable_gate else 0.0)
        
        # Capacidade adaptativa
        adaptation = min(1.0, abs(performance_metrics.get("delta_val_loss", 0.0)) * 50)
        
        # MemÃ³ria temporal
        temporal_memory = min(1.0, self.age * 0.02)
        
        # Auto-evoluÃ§Ã£o
        evolution_success = min(1.0, self.successful_mutations * 0.1)
        
        # Diversidade de ativaÃ§Ã£o (entropia proxy)
        activation_diversity = 0.5  # Base para ativaÃ§Ã£o fixa, pode aumentar com mixed activation
        
        # FÃ³rmula composta
        consciousness = (
            0.2 * arch_complexity +
            0.25 * adaptation +
            0.15 * temporal_memory +
            0.25 * evolution_success +
            0.15 * activation_diversity
        )
        
        self.consciousness_score = max(0.0, min(1.0, consciousness))
        return await self.consciousness_score

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TREINAMENTO E AVALIAÃ‡ÃƒO PRINCIPAL
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def fit_eval(self, workdir: str) -> Dict[str, Any]:
        """
        Executa ciclo completo:
        1. Treino base
        2. Auto-mutaÃ§Ã£o
        3. Treino pÃ³s-mutaÃ§Ã£o
        4. AvaliaÃ§Ã£o OOD
        5. CÃ¡lculo de mÃ©tricas IAÂ³
        """
        logger.info(f"\nğŸ§¬ Iniciando ciclo IAÂ³ para neurÃ´nio {self.neuron_id}")
        logger.info(f"   Idade: {self.age}, MutaÃ§Ãµes: {self.mutation_count}")
        
        os.makedirs(workdir, exist_ok=True)
        self.age += 1
        
        # Gerar dados de treino e validaÃ§Ã£o
        (X_train, y_train), (X_val, y_val) = _make_data(
            n=512, 
            d_in=self.cfg.input_dim, 
            seed=self.cfg.seed + self.age,
            complexity_level=2
        )
        
        # Dados OOD para teste de adaptaÃ§Ã£o
        X_ood, y_ood = _make_ood_data(X_val, y_val, shift_factor=0.2)
        
        self.to(self.cfg.device)
        loss_fn = nn.MSELoss()
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # FASE 1: TREINO BASE
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        logger.info(f"   ğŸ“š Treino base ({self.cfg.steps} steps)...")
        
        optimizer = optim.AdamW(self.parameters(), lr=self.cfg.lr, weight_decay=1e-5)
        
        self.train()
        train_losses = []
        
        for step in range(self.cfg.steps):
            # Batch aleatÃ³rio
            indices = torch.randint(0, X_train.size(0), (self.cfg.batch,))
            x_batch, y_batch = X_train[indices], y_train[indices]
            
            # Forward e backward
            optimizer.zero_grad()
            pred = self.forward(x_batch)
            loss = loss_fn(pred, y_batch)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)
            
            optimizer.step()
            train_losses.append(loss.item())
        
        # ValidaÃ§Ã£o prÃ©-mutaÃ§Ã£o
        self.eval()
        with torch.no_grad():
            val_pred_pre = self.forward(X_val)
            base_val_loss = loss_fn(val_pred_pre, y_val).item()
            
            # OOD baseline
            ood_pred_pre = self.forward(X_ood)
            base_ood_loss = loss_fn(ood_pred_pre, y_ood).item()
        
        logger.info(f"      âœ… Loss treino: {train_losses[-1]:.6f}")
        logger.info(f"      âœ… Loss validaÃ§Ã£o: {base_val_loss:.6f}")
        logger.info(f"      âœ… Loss OOD: {base_ood_loss:.6f}")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # FASE 2: AUTO-MUTAÃ‡ÃƒO
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        logger.info(f"   ğŸ”¬ Executando auto-mutaÃ§Ã£o...")
        
        # Backup do estado atual
        state_backup = self.state_dict()
        config_backup = {
            "act": self.cfg.act,
            "gate_type": type(self.gate).__name__
        }
        
        # Propor mutaÃ§Ãµes
        mutation_result = self.propose_self_mutation()
        mutations_applied = mutation_result["mutations_applied"]
        
        if mutations_applied:
            logger.info(f"      ğŸ”„ MutaÃ§Ãµes aplicadas: {mutations_applied}")
            
            # Treino pÃ³s-mutaÃ§Ã£o (mais curto)
            post_mutation_optimizer = optim.AdamW(self.parameters(), lr=self.cfg.lr * 0.5)
            
            self.train()
            for step in range(self.cfg.steps // 2):  # Metade dos steps
                indices = torch.randint(0, X_train.size(0), (self.cfg.batch,))
                x_batch, y_batch = X_train[indices], y_train[indices]
                
                post_mutation_optimizer.zero_grad()
                pred = self.forward(x_batch)
                loss = loss_fn(pred, y_batch)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)
                post_mutation_optimizer.step()
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # FASE 3: AVALIAÃ‡ÃƒO PÃ“S-MUTAÃ‡ÃƒO
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.eval()
        with torch.no_grad():
            # ValidaÃ§Ã£o pÃ³s-mutaÃ§Ã£o
            val_pred_post = self.forward(X_val)
            post_val_loss = loss_fn(val_pred_post, y_val).item()
            
            # OOD pÃ³s-mutaÃ§Ã£o
            ood_pred_post = self.forward(X_ood)
            post_ood_loss = loss_fn(ood_pred_post, y_ood).item()
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # FASE 4: ANÃLISE DE SUCESSO E NOVIDADE
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        delta_val_loss = post_val_loss - base_val_loss
        delta_ood_loss = post_ood_loss - base_ood_loss
        novelty = self.measure_novelty()
        
        # Determinar sucesso da evoluÃ§Ã£o
        self_evolve_success = (
            delta_val_loss < 0 and  # Melhorou na validaÃ§Ã£o
            delta_ood_loss < 0.05   # NÃ£o piorou muito em OOD
        )
        
        if self_evolve_success:
            self.successful_mutations += 1
            self.update_weight_snapshot()  # Aceitar mudanÃ§as
            logger.info(f"      âœ… Auto-evoluÃ§Ã£o BEM-SUCEDIDA!")
        else:
            # Rollback das mutaÃ§Ãµes se nÃ£o melhoraram
            self.load_state_dict(state_backup)
            self.act = ACTIVATIONS[config_backup["act"]]()
            if config_backup["gate_type"] == "Identity":
                self.gate = nn.Identity()
            else:
                self.gate = nn.Linear(1, 1, bias=False)
            logger.info(f"      ğŸ”„ Auto-evoluÃ§Ã£o falhou, rollback aplicado")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # FASE 5: MÃ‰TRICAS FINAIS IAÂ³
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # Calcular consciÃªncia
        performance_metrics = {
            "delta_val_loss": delta_val_loss,
            "delta_ood_loss": delta_ood_loss,
            "novelty": novelty
        }
        consciousness = self.calculate_consciousness(performance_metrics)
        
        # MÃ©tricas completas
        final_metrics = {
            # IdentificaÃ§Ã£o
            "neuron_id": self.neuron_id,
            "age": self.age,
            "generation": self.generation,
            "timestamp": datetime.utcnow().isoformat() + "Z",
            
            # Performance
            "base_val_loss": base_val_loss,
            "post_val_loss": post_val_loss,
            "delta_val_loss": delta_val_loss,
            "base_ood_loss": base_ood_loss,
            "post_ood_loss": post_ood_loss,
            "delta_ood_loss": delta_ood_loss,
            "best_val": post_val_loss,  # Para compatibilidade
            
            # EvoluÃ§Ã£o
            "novelty": novelty,
            "self_evolve_success": self_evolve_success,
            "mutations_applied": mutations_applied,
            "self_mutations": len(mutations_applied),
            "total_mutations": self.mutation_count,
            "successful_mutations": self.successful_mutations,
            
            # IAÂ³ especÃ­ficos
            "consciousness_score": consciousness,
            "contribution_score": abs(self.get_gate_value()),
            "architectural_changes": len(mutations_applied),
            "synaptic_changes": self.mutation_count,
            "construction_mutations": len([m for m in mutations_applied if "gate" in m]),
            "autonomous_operation": True,  # Operou sem intervenÃ§Ã£o
            
            # Estado arquitetural
            "current_activation": self.cfg.act,
            "has_learnable_gate": isinstance(self.gate, nn.Linear),
            "weight_norm": self.get_weight_norm(),
            "grad_norm": self.get_grad_norm(),
            "architectural_complexity": self.get_architectural_complexity()
        }
        
        # Salvar relatÃ³rio
        with open(os.path.join(workdir, "metrics.json"), "w") as f:
            json.dump(final_metrics, f, indent=2)
        
        # Salvar estado do neurÃ´nio
        torch.save({
            "state_dict": self.state_dict(),
            "config": self.cfg,
            "metadata": {
                "neuron_id": self.neuron_id,
                "age": self.age,
                "mutation_count": self.mutation_count,
                "consciousness_score": consciousness
            }
        }, os.path.join(workdir, "neuron_state.pt"))
        
        logger.info(f"   ğŸ“Š RELATÃ“RIO FINAL:")
        logger.info(f"      Î” Val Loss: {delta_val_loss:.6f}")
        logger.info(f"      Novidade: {novelty:.6f}")
        logger.info(f"      Sucesso evoluÃ§Ã£o: {self_evolve_success}")
        logger.info(f"      ConsciÃªncia: {consciousness:.3f}")
        logger.info(f"      MutaÃ§Ãµes: {len(mutations_applied)}")
        
        return await final_metrics

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MÃ‰TODOS AUXILIARES
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def get_weight_norm(self) -> float:
        """Norma dos pesos do neurÃ´nio"""
        return await float(torch.norm(self.fc.weight, p=2).item())
    
    async def get_grad_norm(self) -> float:
        """Norma dos gradientes do neurÃ´nio"""
        if self.fc.weight.grad is None:
            return await 0.0
        return await float(torch.norm(self.fc.weight.grad, p=2).item())
    
    async def get_gate_value(self) -> float:
        """Valor efetivo do gate"""
        if isinstance(self.gate, nn.Linear):
            return await float(self.gate.weight.item())
        else:
            return await 1.0  # Identity gate
    
    async def get_architectural_complexity(self) -> float:
        """Score de complexidade arquitetural"""
        complexity = 0.5  # Base
        
        # Bonus por gate learnable
        if isinstance(self.gate, nn.Linear):
            complexity += 0.3
        
        # Bonus por ativaÃ§Ã£o nÃ£o-trivial
        if self.cfg.act != "relu":
            complexity += 0.2
        
        return await min(1.0, complexity)

    async def get_summary(self) -> Dict[str, Any]:
        """Retorna resumo completo do neurÃ´nio"""
        return await {
            "id": self.neuron_id,
            "age": self.age,
            "generation": self.generation,
            "activation": self.cfg.act,
            "has_learnable_gate": isinstance(self.gate, nn.Linear),
            "consciousness": self.consciousness_score,
            "total_mutations": self.mutation_count,
            "successful_mutations": self.successful_mutations,
            "weight_norm": self.get_weight_norm(),
            "architectural_complexity": self.get_architectural_complexity(),
            "birth_time": self.birth_time.isoformat()
        }

if __name__ == "__main__":
    import sys
    
    # Teste da cÃ©lula-neurÃ´nio
    if len(sys.argv) > 1 and sys.argv[1] == "--test":
        logger.info("ğŸ§ª Testando cÃ©lula-neurÃ´nio IAÂ³...")
        
        cfg = TrainConfig(
            input_dim=16,
            hidden_dim=16,
            lr=0.01,
            steps=100,
            seed=42,
            batch=32
        )
        
        neuron = IA3Neuron(cfg)
        
        # Executar ciclo completo
        workdir = f"/tmp/neuron_test_{int(time.time())}"
        metrics = neuron.fit_eval(workdir)
        
        logger.info(f"\nğŸ“Š Resultados do teste:")
        logger.info(f"   NeurÃ´nio ID: {metrics['neuron_id']}")
        logger.info(f"   Î” Val Loss: {metrics['delta_val_loss']:.6f}")
        logger.info(f"   Novidade: {metrics['novelty']:.6f}")
        logger.info(f"   Sucesso evoluÃ§Ã£o: {metrics['self_evolve_success']}")
        logger.info(f"   ConsciÃªncia: {metrics['consciousness_score']:.3f}")
        logger.info(f"   MutaÃ§Ãµes: {metrics['self_mutations']}")
        
        # Testar EquaÃ§Ã£o da Morte
        sys.path.append("/root/darwin")
        from rules.equacao_da_morte import equacao_da_morte
        
        death_result = equacao_da_morte(metrics)
        logger.info(f"\nâš–ï¸ EquaÃ§Ã£o da Morte:")
        logger.info(f"   DecisÃ£o: {death_result['decision']}")
        logger.info(f"   E(t+1): {death_result['E']}")
        logger.info(f"   A(t): {death_result['A']}, C(t): {death_result['C']}")
        
        logger.info(f"\nâœ… Teste completo!")
        logger.info(f"   Workdir: {workdir}")
    else:
        logger.info("Uso: python ia3_neuron.py --test")