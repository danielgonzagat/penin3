âœ“ Incompletude Infinita carregada automaticamente
{"timestamp": "2025-10-04 16:50:04", "logger": "unified_brain", "level": "INFO", "message": "ðŸ’» Using CPU", "module": "brain_daemon_real_env", "func": "__init__", "line": 103}
{"timestamp": "2025-10-04 16:50:04", "logger": "unified_brain", "level": "INFO", "message": "âœ… Experience Replay ativo (50K capacity)", "module": "brain_daemon_real_env", "func": "__init__", "line": 166}
{"timestamp": "2025-10-04 16:50:04", "logger": "unified_brain", "level": "INFO", "message": "âœ… Auto-Tuner ativo", "module": "brain_daemon_real_env", "func": "__init__", "line": 177}
{"timestamp": "2025-10-04 16:50:04", "logger": "unified_brain", "level": "INFO", "message": "ðŸ”“ Removing stale lock file", "module": "brain_daemon_real_env", "func": "_acquire_lock", "line": 211}
{"timestamp": "2025-10-04 16:50:04", "logger": "unified_brain", "level": "INFO", "message": "ðŸ”’ Lock acquired (PID 418744)", "module": "brain_daemon_real_env", "func": "_acquire_lock", "line": 216}
{"timestamp": "2025-10-04 16:50:04", "logger": "unified_brain", "level": "INFO", "message": "ðŸŽ“ Curriculum initialized: 4 stages", "module": "brain_daemon_real_env", "func": "__init__", "line": 188}
{"timestamp": "2025-10-04 16:50:04", "logger": "unified_brain", "level": "INFO", "message": "ðŸš€ Brain V3 EMERGENCIAL v3.0.0.4c418f9: 12,288x speedup target", "module": "brain_daemon_real_env", "func": "__init__", "line": 190}
{"timestamp": "2025-10-04 16:50:04", "logger": "unified_brain", "level": "INFO", "message": "Loading brain...", "module": "brain_daemon_real_env", "func": "initialize", "line": 236}
{"timestamp": "2025-10-04 16:50:09", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_neuron_00103: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:09", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_neuron_00185: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:09", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_neuron_00119: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:09", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_neuron_g36_output_077: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:09", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_neuron_g36_output_075: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:09", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_neuron_g36_output_004: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:10", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_api_xai_ia3_neurons_600_updated_20250924_160347_0016: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:10", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_api_deepseek_ia3_neurons_600_updated_20250924_162652_0031: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:10", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_api_openai_ia3_neurons_20250924_141200_0081: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:10", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen1_child_2388: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:10", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen1_child_2409: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:10", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen1_child_2425: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:10", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen1_child_2472: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:10", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen1_child_2481: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:10", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen2_child_2729: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:10", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen3_child_2794: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:10", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen3_child_2825: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:10", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen3_child_2869: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:10", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen3_child_2909: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:10", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen3_child_2914: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:11", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen4_child_2993: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:11", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen4_child_3083: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:11", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen6_child_3398: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:11", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen6_child_3497: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:11", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen7_child_3563: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:11", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen7_child_3704: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:11", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen8_child_3833: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:11", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen9_child_4046: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:11", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen10_child_4457: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:11", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen11_child_4743: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:11", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen12_child_4988: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:12", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen12_child_5037: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:12", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen13_child_5389: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:12", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen14_child_5649: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:12", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen14_child_5657: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:12", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen14_child_5691: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:12", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen15_child_5752: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:12", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen15_child_5755: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:12", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen15_child_5960: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:12", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen15_child_5984: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:12", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen15_child_6006: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:12", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen16_child_6356: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:12", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen17_child_6519: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:12", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen17_child_6570: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:12", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen17_child_6712: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:12", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen18_child_6948: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:13", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen18_child_7102: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:13", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen18_child_7154: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:13", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen19_child_7207: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:13", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen19_child_7448: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:13", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen19_child_7466: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:13", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen19_child_7482: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:13", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen19_child_7489: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:13", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen19_child_7549: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:13", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen20_child_8028: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:13", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen21_child_8156: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:13", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen21_child_8293: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:13", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen21_child_8322: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:13", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen21_child_8335: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:13", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen21_child_8408: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:13", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen21_child_8413: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:14", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen21_child_8504: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:14", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen22_child_8619: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:14", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen22_child_8830: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:14", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen22_child_8865: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:14", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen22_child_8909: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:14", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen22_child_8996: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:14", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen23_child_9202: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:14", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen23_child_9226: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:14", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen23_child_9254: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:14", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen23_child_9400: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:14", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen23_child_9408: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:14", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen23_child_9502: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:15", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen23_child_9504: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:15", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen24_child_9590: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:15", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen24_child_9786: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:15", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen24_child_9927: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:15", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen24_child_9985: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:15", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen25_child_10505: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:15", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen25_child_10676: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:15", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen26_child_10841: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:15", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen26_child_10843: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:15", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen26_child_10862: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:15", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen26_child_10926: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:15", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen26_child_11073: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:15", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen26_child_11121: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:15", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen26_child_11168: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:15", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen26_child_11237: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:16", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11332: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:16", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11599: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:16", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11634: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:16", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11654: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:16", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11693: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:16", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11729: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:16", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11746: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:16", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11777: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:16", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11797: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:16", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11947: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:16", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_11953: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:16", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12032: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:16", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12057: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:16", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12074: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:16", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12127: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:16", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12133: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:16", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12143: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:17", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12221: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:17", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12240: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:17", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12335: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:17", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12369: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:17", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12515: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:17", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12538: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:17", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen29_child_12777: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:17", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen29_child_12856: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:17", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen29_child_12971: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:17", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen29_child_12980: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:17", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen29_child_13055: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:17", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen29_child_13170: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:17", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen29_child_13246: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:17", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen29_child_13275: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:17", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen29_child_13291: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:17", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13617: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:17", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13639: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:18", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13650: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:18", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13661: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:18", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13743: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:18", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13767: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:18", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13852: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:18", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13950: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:18", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13955: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:18", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13992: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:18", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_14040: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:18", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_14092: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:18", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14159: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:18", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14221: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:18", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14329: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:18", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14401: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:18", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14410: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:19", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14411: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:19", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14467: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:19", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14479: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:19", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14637: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:19", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14726: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:19", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14818: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:19", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen32_child_14935: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:19", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen32_child_15057: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:19", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen32_child_15200: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:19", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen32_child_15461: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:19", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen32_child_15497: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:19", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen32_child_15573: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:20", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen32_child_15641: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:20", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen32_child_15662: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:20", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_15968: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:20", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_16017: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:20", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_16026: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:20", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_16061: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:20", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_16082: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:20", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_16214: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:20", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_16252: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:20", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_16268: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:20", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_16510: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:21", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_16523: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:21", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_16734: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:21", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_16777: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:21", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_16781: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:21", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_16861: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:21", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_16922: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:21", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_16955: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:21", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_17086: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:21", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_17119: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:22", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_17173: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:22", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_17228: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:22", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_17412: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:22", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen35_child_17511: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:22", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen35_child_17699: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:22", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen35_child_17909: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:22", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen35_child_18084: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:22", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen35_child_18195: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:22", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_18397: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:22", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_18455: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:22", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_18680: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:22", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_18686: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:22", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_18696: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:22", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_18777: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:23", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_18806: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:23", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_18873: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:23", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_18878: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:23", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_19072: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:23", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_19137: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:23", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_19143: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:23", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_19201: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:23", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_19308: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:23", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_19450: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:23", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_19585: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:23", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_19587: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:23", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_19713: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:24", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_19775: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:24", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_19787: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:24", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_19847: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:24", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_19889: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:24", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_19968: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:24", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_20052: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:24", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_20054: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:24", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_20077: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:24", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20448: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:24", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20454: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:24", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20480: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20490: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20494: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20495: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20795: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20950: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20955: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20986: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_21012: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_21058: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_21129: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_21374: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_21659: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_21741: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_21797: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_21932: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_21975: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22042: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22134: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22170: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22357: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22361: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22362: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:25", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22372: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22394: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22404: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22457: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22480: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22531: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22578: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22626: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_22850: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_22902: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_22951: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23187: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23188: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23192: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23203: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23212: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23243: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23400: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23480: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23560: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23583: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23586: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23629: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23670: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23720: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23832: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 426}
{"timestamp": "2025-10-04 16:50:26", "logger": "unified_brain", "level": "INFO", "message": "Loaded 254 neurons with adapters from /root/UNIFIED_BRAIN/snapshots/initial_state_registry_adapters.pt", "module": "brain_spec", "func": "load_with_adapters", "line": 437}
{"timestamp": "2025-10-04 16:50:27", "logger": "unified_brain", "level": "INFO", "message": "Found 254 neurons, limiting to 4 for max speedup...", "module": "brain_daemon_real_env", "func": "initialize", "line": 246}
{"timestamp": "2025-10-04 16:50:27", "logger": "unified_brain", "level": "INFO", "message": "âœ… Using top 4 neurons (4x speedup)", "module": "brain_daemon_real_env", "func": "initialize", "line": 257}
{"timestamp": "2025-10-04 16:50:27", "logger": "unified_brain", "level": "INFO", "message": "Router initialized: 4 active neurons", "module": "unified_brain_core", "func": "initialize_router", "line": 132}
{"timestamp": "2025-10-04 16:50:27", "logger": "unified_brain", "level": "INFO", "message": "âœ… Brain loaded: 4 active neurons", "module": "brain_daemon_real_env", "func": "initialize", "line": 265}
{"timestamp": "2025-10-04 16:50:27", "logger": "unified_brain", "level": "INFO", "message": "ðŸš€ Attempting torch.compile JIT optimization...", "module": "brain_daemon_real_env", "func": "initialize", "line": 272}
{"timestamp": "2025-10-04 16:50:27", "logger": "unified_brain", "level": "WARNING", "message": "torch.compile failed (continuing without): 'NoneType' object has no attribute 'v7_bridge'", "module": "brain_daemon_real_env", "func": "initialize", "line": 280}
{"timestamp": "2025-10-04 16:50:30", "logger": "unified_brain", "level": "INFO", "message": "âœ… Optimizer: 44 params", "module": "brain_daemon_real_env", "func": "initialize", "line": 345}
{"timestamp": "2025-10-04 16:50:30", "logger": "unified_brain", "level": "WARNING", "message": "WORM chain rotated due to invalid integrity", "module": "brain_daemon_real_env", "func": "initialize", "line": 352}
{"timestamp": "2025-10-04 16:50:30", "logger": "unified_brain", "level": "ERROR", "message": "Integrity: mismatch for /root/UNIFIED_BRAIN/real_env_checkpoint_v3.json", "module": "brain_daemon_real_env", "func": "verify_checkpoint_integrity", "line": 1153}
{"timestamp": "2025-10-04 16:50:30", "logger": "unified_brain", "level": "INFO", "message": "âœ… Curiosity module initialized (has its own optimizer)", "module": "brain_daemon_real_env", "func": "initialize", "line": 365}
{"timestamp": "2025-10-04 16:50:30", "logger": "unified_brain", "level": "INFO", "message": "âœ… Self-Analysis module initialized", "module": "brain_daemon_real_env", "func": "initialize", "line": 368}
{"timestamp": "2025-10-04 16:50:30", "logger": "unified_brain", "level": "INFO", "message": "âœ… Metrics Dashboard initialized", "module": "brain_daemon_real_env", "func": "initialize", "line": 369}
{"timestamp": "2025-10-04 16:50:30", "logger": "unified_brain", "level": "INFO", "message": "âœ… Module Synthesizer initialized", "module": "brain_daemon_real_env", "func": "initialize", "line": 376}
{"timestamp": "2025-10-04 16:50:30", "logger": "unified_brain", "level": "INFO", "message": "âœ… Recursive Improvement Engine initialized", "module": "brain_daemon_real_env", "func": "initialize", "line": 377}
{"timestamp": "2025-10-04 16:50:30", "logger": "unified_brain", "level": "INFO", "message": "âœ… Self-Improvement Loop initialized", "module": "brain_daemon_real_env", "func": "initialize", "line": 378}
{"timestamp": "2025-10-04 16:50:30", "logger": "unified_brain", "level": "INFO", "message": "âœ… Ready for FAST learning!", "module": "brain_daemon_real_env", "func": "initialize", "line": 385}
{"timestamp": "2025-10-04 16:50:31", "logger": "unified_brain", "level": "INFO", "message": "================================================================================", "module": "brain_daemon_real_env", "func": "run", "line": 1166}
{"timestamp": "2025-10-04 16:50:31", "logger": "unified_brain", "level": "INFO", "message": "ðŸš€ V3 EMERGENCIAL: 12,288x SPEEDUP", "module": "brain_daemon_real_env", "func": "run", "line": 1167}
{"timestamp": "2025-10-04 16:50:31", "logger": "unified_brain", "level": "INFO", "message": "================================================================================", "module": "brain_daemon_real_env", "func": "run", "line": 1168}
{"timestamp": "2025-10-04 16:50:31", "logger": "unified_brain", "level": "INFO", "message": "Device: cpu", "module": "brain_daemon_real_env", "func": "run", "line": 1169}
{"timestamp": "2025-10-04 16:50:31", "logger": "unified_brain", "level": "INFO", "message": "Optimizations:", "module": "brain_daemon_real_env", "func": "run", "line": 1170}
{"timestamp": "2025-10-04 16:50:31", "logger": "unified_brain", "level": "INFO", "message": "  â€¢ 254â†’16 neurons (16x)", "module": "brain_daemon_real_env", "func": "run", "line": 1171}
{"timestamp": "2025-10-04 16:50:31", "logger": "unified_brain", "level": "INFO", "message": "  â€¢ 4â†’1 brain steps (4x)", "module": "brain_daemon_real_env", "func": "run", "line": 1172}
{"timestamp": "2025-10-04 16:50:31", "logger": "unified_brain", "level": "INFO", "message": "  â€¢ Top-k 128â†’8 (16x)", "module": "brain_daemon_real_env", "func": "run", "line": 1173}
{"timestamp": "2025-10-04 16:50:31", "logger": "unified_brain", "level": "INFO", "message": "  â€¢ Batch training (2x)", "module": "brain_daemon_real_env", "func": "run", "line": 1174}
{"timestamp": "2025-10-04 16:50:31", "logger": "unified_brain", "level": "INFO", "message": "  â€¢ Total: 12,288x faster!", "module": "brain_daemon_real_env", "func": "run", "line": 1175}
{"timestamp": "2025-10-04 16:50:31", "logger": "unified_brain", "level": "INFO", "message": "================================================================================", "module": "brain_daemon_real_env", "func": "run", "line": 1176}
{"timestamp": "2025-10-04 16:50:31", "logger": "unified_brain", "level": "INFO", "message": "Phase 1 hooks enabled: GÃ¶del monitor and Needle meta-controller", "module": "brain_daemon_real_env", "func": "run", "line": 1178}
{"timestamp": "2025-10-04 16:52:02", "logger": "unified_brain", "level": "INFO", "message": "ðŸŽŠ NEW BEST: 15.0!", "module": "brain_daemon_real_env", "func": "run_episode", "line": 559}
{"timestamp": "2025-10-04 16:52:02", "logger": "unified_brain", "level": "ERROR", "message": "Error: sequence index must be integer, not 'slice'", "module": "brain_daemon_real_env", "func": "run", "line": 1185}
Traceback (most recent call last):
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 1182, in run
    episode_reward = self.run_episode()
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 562, in run_episode
    recent = self.stats['rewards'][-100:]
TypeError: sequence index must be integer, not 'slice'
{"timestamp": "2025-10-04 16:54:26", "logger": "unified_brain", "level": "INFO", "message": "ðŸŽŠ NEW BEST: 17.0!", "module": "brain_daemon_real_env", "func": "run_episode", "line": 559}
{"timestamp": "2025-10-04 16:54:26", "logger": "unified_brain", "level": "ERROR", "message": "Error: sequence index must be integer, not 'slice'", "module": "brain_daemon_real_env", "func": "run", "line": 1185}
Traceback (most recent call last):
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 1182, in run
    episode_reward = self.run_episode()
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 562, in run_episode
    recent = self.stats['rewards'][-100:]
TypeError: sequence index must be integer, not 'slice'
âœ“ Incompletude Infinita carregada automaticamente
{"timestamp": "2025-10-04 17:46:24", "logger": "unified_brain", "level": "INFO", "message": "ðŸ’» Using CPU", "module": "brain_daemon_real_env", "func": "__init__", "line": 103}
{"timestamp": "2025-10-04 17:46:24", "logger": "unified_brain", "level": "INFO", "message": "âœ… Telemetria completa ativa (DB + Emergence)", "module": "brain_daemon_real_env", "func": "__init__", "line": 194}
{"timestamp": "2025-10-04 17:46:24", "logger": "unified_brain", "level": "INFO", "message": "âœ… Experience Replay ativo (50K capacity)", "module": "brain_daemon_real_env", "func": "__init__", "line": 205}
{"timestamp": "2025-10-04 17:46:24", "logger": "unified_brain", "level": "INFO", "message": "âœ… Auto-Tuner ativo", "module": "brain_daemon_real_env", "func": "__init__", "line": 216}
{"timestamp": "2025-10-04 17:46:24", "logger": "unified_brain", "level": "INFO", "message": "ðŸ”’ Lock acquired (PID 690017)", "module": "brain_daemon_real_env", "func": "_acquire_lock", "line": 255}
{"timestamp": "2025-10-04 17:46:24", "logger": "unified_brain", "level": "INFO", "message": "ðŸŽ“ Curriculum initialized: 4 stages", "module": "brain_daemon_real_env", "func": "__init__", "line": 227}
{"timestamp": "2025-10-04 17:46:24", "logger": "unified_brain", "level": "INFO", "message": "ðŸš€ Brain V3 EMERGENCIAL v3.0.0.4c418f9: 12,288x speedup target", "module": "brain_daemon_real_env", "func": "__init__", "line": 229}
{"timestamp": "2025-10-04 17:46:24", "logger": "unified_brain", "level": "INFO", "message": "Loading brain...", "module": "brain_daemon_real_env", "func": "initialize", "line": 275}
{"timestamp": "2025-10-04 17:46:29", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_neuron_00103: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:30", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_neuron_00185: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:30", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_neuron_00119: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:30", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_neuron_g36_output_077: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:30", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_neuron_g36_output_075: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:30", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_neuron_g36_output_004: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:30", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_api_xai_ia3_neurons_600_updated_20250924_160347_0016: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:30", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_api_deepseek_ia3_neurons_600_updated_20250924_162652_0031: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:30", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_api_openai_ia3_neurons_20250924_141200_0081: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:30", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen1_child_2388: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:30", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen1_child_2409: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:30", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen1_child_2425: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:30", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen1_child_2472: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:30", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen1_child_2481: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:30", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen2_child_2729: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:31", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen3_child_2794: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:31", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen3_child_2825: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:31", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen3_child_2869: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:31", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen3_child_2909: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:31", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen3_child_2914: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:31", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen4_child_2993: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:31", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen4_child_3083: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:31", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen6_child_3398: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:31", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen6_child_3497: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:31", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen7_child_3563: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:31", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen7_child_3704: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:31", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen8_child_3833: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:31", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen9_child_4046: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:31", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen10_child_4457: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:31", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen11_child_4743: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:31", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen12_child_4988: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:31", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen12_child_5037: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:31", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen13_child_5389: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen14_child_5649: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen14_child_5657: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen14_child_5691: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen15_child_5752: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen15_child_5755: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen15_child_5960: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen15_child_5984: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen15_child_6006: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen16_child_6356: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen17_child_6519: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen17_child_6570: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen17_child_6712: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen18_child_6948: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen18_child_7102: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen18_child_7154: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen19_child_7207: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen19_child_7448: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen19_child_7466: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:32", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen19_child_7482: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:33", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen19_child_7489: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:33", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen19_child_7549: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:33", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen20_child_8028: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:33", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen21_child_8156: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:33", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen21_child_8293: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:33", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen21_child_8322: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:33", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen21_child_8335: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:33", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen21_child_8408: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:33", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen21_child_8413: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:33", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen21_child_8504: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:33", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen22_child_8619: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:33", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen22_child_8830: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:33", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen22_child_8865: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:33", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen22_child_8909: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:33", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen22_child_8996: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:33", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen23_child_9202: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:33", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen23_child_9226: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen23_child_9254: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen23_child_9400: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen23_child_9408: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen23_child_9502: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen23_child_9504: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen24_child_9590: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen24_child_9786: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen24_child_9927: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen24_child_9985: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen25_child_10505: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen25_child_10676: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen26_child_10841: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen26_child_10843: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen26_child_10862: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen26_child_10926: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen26_child_11073: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen26_child_11121: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen26_child_11168: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen26_child_11237: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11332: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:34", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11599: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:35", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11634: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:35", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11654: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:35", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11693: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:35", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11729: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:35", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11746: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:35", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11777: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:35", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11797: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:35", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen27_child_11947: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:35", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_11953: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:35", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12032: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:35", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12057: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:35", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12074: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:35", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12127: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:35", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12133: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:35", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12143: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:35", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12221: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:36", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12240: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:36", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12335: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:36", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12369: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:36", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12515: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:36", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen28_child_12538: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:36", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen29_child_12777: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:36", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen29_child_12856: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:36", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen29_child_12971: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:36", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen29_child_12980: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:36", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen29_child_13055: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:36", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen29_child_13170: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:36", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen29_child_13246: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:36", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen29_child_13275: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:36", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen29_child_13291: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:36", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13617: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:36", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13639: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:37", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13650: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:37", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13661: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:37", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13743: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:37", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13767: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:37", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13852: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:37", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13950: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:37", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13955: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:37", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_13992: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:37", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_14040: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:37", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen30_child_14092: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:37", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14159: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:37", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14221: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:37", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14329: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:37", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14401: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:38", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14410: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:38", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14411: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:38", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14467: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:38", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14479: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:38", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14637: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:38", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14726: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:38", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen31_child_14818: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:38", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen32_child_14935: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:38", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen32_child_15057: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:38", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen32_child_15200: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:38", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen32_child_15461: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:38", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen32_child_15497: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:38", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen32_child_15573: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:38", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen32_child_15641: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:38", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen32_child_15662: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:38", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_15968: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:38", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_16017: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:38", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_16026: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_16061: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_16082: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_16214: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_16252: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_16268: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_16510: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen33_child_16523: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_16734: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_16777: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_16781: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_16861: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_16922: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_16955: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_17086: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_17119: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_17173: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_17228: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen34_child_17412: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen35_child_17511: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen35_child_17699: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen35_child_17909: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen35_child_18084: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:39", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen35_child_18195: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:40", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_18397: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:40", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_18455: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:40", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_18680: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:40", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_18686: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:40", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_18696: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:40", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_18777: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:40", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_18806: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:40", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_18873: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:40", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_18878: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:40", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_19072: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:40", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_19137: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:40", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_19143: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:40", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_19201: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:40", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen36_child_19308: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_19450: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_19585: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_19587: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_19713: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_19775: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_19787: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_19847: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_19889: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_19968: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_20052: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_20054: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen37_child_20077: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20448: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20454: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20480: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20490: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20494: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20495: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:41", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20795: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20950: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20955: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_20986: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_21012: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_21058: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_21129: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen38_child_21374: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_21659: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_21741: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_21797: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_21932: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_21975: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22042: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22134: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22170: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22357: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22361: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22362: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22372: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22394: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22404: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22457: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22480: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22531: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22578: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen39_child_22626: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_22850: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_22902: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_22951: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23187: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23188: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23192: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23203: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23212: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:42", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23243: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:43", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23400: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:43", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23480: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:43", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23560: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:43", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23583: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:43", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23586: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:43", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23629: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:43", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23670: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:43", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23720: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:43", "logger": "unified_brain", "level": "WARNING", "message": "Neuron darwin_gen40_child_23832: usando adapters novos (compat falhou: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).)", "module": "brain_spec", "func": "load_with_adapters", "line": 440}
{"timestamp": "2025-10-04 17:46:43", "logger": "unified_brain", "level": "INFO", "message": "Loaded 254 neurons with adapters from /root/UNIFIED_BRAIN/snapshots/initial_state_registry_adapters.pt", "module": "brain_spec", "func": "load_with_adapters", "line": 454}
{"timestamp": "2025-10-04 17:46:43", "logger": "unified_brain", "level": "INFO", "message": "Found 0 neurons, limiting to 4 for max speedup...", "module": "brain_daemon_real_env", "func": "initialize", "line": 285}
{"timestamp": "2025-10-04 17:46:43", "logger": "unified_brain", "level": "INFO", "message": "Router initialized: 8 active neurons", "module": "unified_brain_core", "func": "initialize_router", "line": 132}
{"timestamp": "2025-10-04 17:46:43", "logger": "unified_brain", "level": "INFO", "message": "âœ… Brain loaded: 0 active neurons", "module": "brain_daemon_real_env", "func": "initialize", "line": 304}
{"timestamp": "2025-10-04 17:46:43", "logger": "unified_brain", "level": "INFO", "message": "ðŸš€ Attempting torch.compile JIT optimization...", "module": "brain_daemon_real_env", "func": "initialize", "line": 311}
{"timestamp": "2025-10-04 17:46:43", "logger": "unified_brain", "level": "WARNING", "message": "torch.compile failed (continuing without): 'NoneType' object has no attribute 'v7_bridge'", "module": "brain_daemon_real_env", "func": "initialize", "line": 319}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "âœ… Optimizer: 28 params", "module": "brain_daemon_real_env", "func": "initialize", "line": 384}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "WARNING", "message": "WORM chain rotated due to invalid integrity", "module": "brain_daemon_real_env", "func": "initialize", "line": 391}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "âœ… Curiosity module initialized (has its own optimizer)", "module": "brain_daemon_real_env", "func": "initialize", "line": 404}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "âœ… Self-Analysis module initialized", "module": "brain_daemon_real_env", "func": "initialize", "line": 407}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "âœ… Metrics Dashboard initialized", "module": "brain_daemon_real_env", "func": "initialize", "line": 408}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "âœ… Module Synthesizer initialized", "module": "brain_daemon_real_env", "func": "initialize", "line": 415}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "âœ… Recursive Improvement Engine initialized", "module": "brain_daemon_real_env", "func": "initialize", "line": 416}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "âœ… Self-Improvement Loop initialized", "module": "brain_daemon_real_env", "func": "initialize", "line": 417}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "âœ… Ready for FAST learning!", "module": "brain_daemon_real_env", "func": "initialize", "line": 424}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "================================================================================", "module": "brain_daemon_real_env", "func": "run", "line": 1407}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "ðŸš€ V3 EMERGENCIAL: 12,288x SPEEDUP", "module": "brain_daemon_real_env", "func": "run", "line": 1408}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "================================================================================", "module": "brain_daemon_real_env", "func": "run", "line": 1409}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "Device: cpu", "module": "brain_daemon_real_env", "func": "run", "line": 1410}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "Optimizations:", "module": "brain_daemon_real_env", "func": "run", "line": 1411}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "  â€¢ 254â†’16 neurons (16x)", "module": "brain_daemon_real_env", "func": "run", "line": 1412}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "  â€¢ 4â†’1 brain steps (4x)", "module": "brain_daemon_real_env", "func": "run", "line": 1413}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "  â€¢ Top-k 128â†’8 (16x)", "module": "brain_daemon_real_env", "func": "run", "line": 1414}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "  â€¢ Batch training (2x)", "module": "brain_daemon_real_env", "func": "run", "line": 1415}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "  â€¢ Total: 12,288x faster!", "module": "brain_daemon_real_env", "func": "run", "line": 1416}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "================================================================================", "module": "brain_daemon_real_env", "func": "run", "line": 1417}
{"timestamp": "2025-10-04 17:46:45", "logger": "unified_brain", "level": "INFO", "message": "Phase 1 hooks enabled: GÃ¶del monitor and Needle meta-controller", "module": "brain_daemon_real_env", "func": "run", "line": 1419}
{"timestamp": "2025-10-04 17:46:46", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.463s, obs=0.000s, fwd=0.441s (controller: penin=0.0ms, fwd=440.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:46:46", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.370s, obs=0.000s, fwd=0.349s (controller: penin=0.0ms, fwd=348.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:46:47", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.372s, obs=0.000s, fwd=0.341s (controller: penin=0.0ms, fwd=340.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:46:47", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.347s, obs=0.000s, fwd=0.325s (controller: penin=0.0ms, fwd=325.3ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:46:47", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.451s, obs=0.000s, fwd=0.418s (controller: penin=0.0ms, fwd=407.2ms, darwin=0.0ms), act=0.012s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:46:48", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.359s, obs=0.000s, fwd=0.337s (controller: penin=0.0ms, fwd=337.4ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:46:48", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.383s, obs=0.000s, fwd=0.362s (controller: penin=0.0ms, fwd=361.8ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:46:48", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.329s, obs=0.000s, fwd=0.308s (controller: penin=0.0ms, fwd=308.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:46:49", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.423s, obs=0.000s, fwd=0.401s (controller: penin=0.0ms, fwd=401.4ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:46:49", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.426s, obs=0.000s, fwd=0.404s (controller: penin=0.0ms, fwd=372.8ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:46:50", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.424s, obs=0.000s, fwd=0.402s (controller: penin=0.0ms, fwd=401.8ms, darwin=0.3ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:46:50", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.367s, obs=0.000s, fwd=0.338s (controller: penin=0.0ms, fwd=337.8ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:46:50", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.368s, obs=0.000s, fwd=0.345s (controller: penin=0.0ms, fwd=344.5ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:46:51", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.354s, obs=0.000s, fwd=0.332s (controller: penin=0.0ms, fwd=332.2ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:46:51", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.392s, obs=0.000s, fwd=0.370s (controller: penin=0.0ms, fwd=362.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:47:33", "logger": "unified_brain", "level": "ERROR", "message": "Error: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.", "module": "brain_daemon_real_env", "func": "run", "line": 1433}
Traceback (most recent call last):
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 1426, in run
    episode_reward = self.run_episode()
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 619, in run_episode
    loss_value = self.train_on_episode(ep_states, ep_actions, ep_rewards, ep_values, ep_log_probs)
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 980, in train_on_episode
    logits_new, values_new, _ = self.controller.v7_bridge(states_batch)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/UNIFIED_BRAIN/brain_system_integration.py", line 69, in forward
    logits = self.action_head(Zf)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 244, in forward
    input = module(input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.
{"timestamp": "2025-10-04 17:47:34", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.352s, obs=0.000s, fwd=0.325s (controller: penin=0.0ms, fwd=325.3ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:47:34", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.384s, obs=0.000s, fwd=0.349s (controller: penin=0.0ms, fwd=349.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:47:35", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.513s, obs=0.000s, fwd=0.492s (controller: penin=0.0ms, fwd=491.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:47:35", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.610s, obs=0.000s, fwd=0.588s (controller: penin=0.0ms, fwd=587.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:47:36", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.398s, obs=0.000s, fwd=0.376s (controller: penin=0.0ms, fwd=364.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:47:36", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.348s, obs=0.000s, fwd=0.327s (controller: penin=0.0ms, fwd=326.3ms, darwin=0.3ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:47:36", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.354s, obs=0.000s, fwd=0.330s (controller: penin=0.0ms, fwd=330.4ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:47:37", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.329s, obs=0.000s, fwd=0.308s (controller: penin=0.0ms, fwd=307.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:47:37", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.357s, obs=0.000s, fwd=0.336s (controller: penin=0.0ms, fwd=335.5ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:47:37", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.428s, obs=0.000s, fwd=0.401s (controller: penin=0.0ms, fwd=360.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:47:38", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.352s, obs=0.000s, fwd=0.330s (controller: penin=0.0ms, fwd=330.3ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:47:38", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.360s, obs=0.000s, fwd=0.339s (controller: penin=0.0ms, fwd=338.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:15", "logger": "unified_brain", "level": "ERROR", "message": "Error: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.", "module": "brain_daemon_real_env", "func": "run", "line": 1433}
Traceback (most recent call last):
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 1426, in run
    episode_reward = self.run_episode()
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 619, in run_episode
    loss_value = self.train_on_episode(ep_states, ep_actions, ep_rewards, ep_values, ep_log_probs)
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 980, in train_on_episode
    logits_new, values_new, _ = self.controller.v7_bridge(states_batch)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/UNIFIED_BRAIN/brain_system_integration.py", line 69, in forward
    logits = self.action_head(Zf)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 244, in forward
    input = module(input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.
{"timestamp": "2025-10-04 17:48:16", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.488s, obs=0.000s, fwd=0.458s (controller: penin=0.0ms, fwd=457.8ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:16", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.396s, obs=0.000s, fwd=0.374s (controller: penin=0.0ms, fwd=374.3ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:16", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.315s, obs=0.000s, fwd=0.293s (controller: penin=0.0ms, fwd=285.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:17", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.399s, obs=0.000s, fwd=0.374s (controller: penin=0.0ms, fwd=374.1ms, darwin=0.3ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:17", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.386s, obs=0.000s, fwd=0.362s (controller: penin=0.0ms, fwd=361.5ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:17", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.344s, obs=0.000s, fwd=0.322s (controller: penin=0.0ms, fwd=322.4ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:18", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.554s, obs=0.000s, fwd=0.533s (controller: penin=0.0ms, fwd=533.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:18", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.468s, obs=0.000s, fwd=0.446s (controller: penin=0.0ms, fwd=429.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:19", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.334s, obs=0.000s, fwd=0.313s (controller: penin=0.0ms, fwd=312.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:19", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.374s, obs=0.000s, fwd=0.352s (controller: penin=0.0ms, fwd=352.4ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:20", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.401s, obs=0.000s, fwd=0.380s (controller: penin=0.0ms, fwd=379.8ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:20", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.343s, obs=0.000s, fwd=0.321s (controller: penin=0.0ms, fwd=321.3ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:20", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.401s, obs=0.000s, fwd=0.371s (controller: penin=0.0ms, fwd=365.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:21", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.398s, obs=0.000s, fwd=0.376s (controller: penin=0.0ms, fwd=375.9ms, darwin=0.2ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:21", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.393s, obs=0.000s, fwd=0.372s (controller: penin=0.0ms, fwd=371.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:21", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.427s, obs=0.000s, fwd=0.406s (controller: penin=0.0ms, fwd=405.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:22", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.394s, obs=0.000s, fwd=0.368s (controller: penin=0.0ms, fwd=367.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:22", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.382s, obs=0.000s, fwd=0.360s (controller: penin=0.0ms, fwd=344.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:23", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.366s, obs=0.000s, fwd=0.345s (controller: penin=0.0ms, fwd=344.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:23", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.328s, obs=0.000s, fwd=0.306s (controller: penin=0.0ms, fwd=306.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:23", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.459s, obs=0.000s, fwd=0.438s (controller: penin=0.0ms, fwd=438.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:24", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.375s, obs=0.000s, fwd=0.353s (controller: penin=0.0ms, fwd=353.3ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:48:24", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.462s, obs=0.000s, fwd=0.441s (controller: penin=0.0ms, fwd=348.8ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:31", "logger": "unified_brain", "level": "ERROR", "message": "Error: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.", "module": "brain_daemon_real_env", "func": "run", "line": 1433}
Traceback (most recent call last):
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 1426, in run
    brain_logger.info("Optimizations:")
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 619, in run_episode
    loss_value = self.train_on_episode(ep_states, ep_actions, ep_rewards, ep_values, ep_log_probs)
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 980, in train_on_episode
    logits_new, values_new, _ = self.controller.v7_bridge(states_batch)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/UNIFIED_BRAIN/brain_system_integration.py", line 69, in forward
    logits = self.action_head(Zf)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 244, in forward
    input = module(input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.
{"timestamp": "2025-10-04 17:49:32", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.354s, obs=0.000s, fwd=0.332s (controller: penin=0.0ms, fwd=332.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:32", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.266s, obs=0.000s, fwd=0.245s (controller: penin=0.0ms, fwd=244.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:32", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.368s, obs=0.000s, fwd=0.347s (controller: penin=0.0ms, fwd=347.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:33", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.345s, obs=0.000s, fwd=0.324s (controller: penin=0.0ms, fwd=323.5ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:33", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.367s, obs=0.000s, fwd=0.340s (controller: penin=0.0ms, fwd=318.3ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:34", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.328s, obs=0.000s, fwd=0.306s (controller: penin=0.0ms, fwd=306.3ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:34", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.338s, obs=0.000s, fwd=0.317s (controller: penin=0.0ms, fwd=316.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:34", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.519s, obs=0.000s, fwd=0.497s (controller: penin=0.0ms, fwd=497.4ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:35", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.349s, obs=0.000s, fwd=0.328s (controller: penin=0.0ms, fwd=327.5ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:35", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.341s, obs=0.000s, fwd=0.320s (controller: penin=0.0ms, fwd=289.3ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:35", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.337s, obs=0.000s, fwd=0.313s (controller: penin=0.0ms, fwd=312.6ms, darwin=0.2ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:36", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.484s, obs=0.000s, fwd=0.462s (controller: penin=0.0ms, fwd=462.3ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:36", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.465s, obs=0.000s, fwd=0.444s (controller: penin=0.0ms, fwd=443.5ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:37", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.386s, obs=0.000s, fwd=0.365s (controller: penin=0.0ms, fwd=364.8ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:37", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.312s, obs=0.000s, fwd=0.291s (controller: penin=0.0ms, fwd=283.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:37", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.364s, obs=0.000s, fwd=0.340s (controller: penin=0.0ms, fwd=339.8ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:38", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.387s, obs=0.000s, fwd=0.366s (controller: penin=0.0ms, fwd=365.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:38", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.355s, obs=0.000s, fwd=0.334s (controller: penin=0.0ms, fwd=334.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:38", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.279s, obs=0.000s, fwd=0.258s (controller: penin=0.0ms, fwd=258.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:39", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.373s, obs=0.000s, fwd=0.350s (controller: penin=0.0ms, fwd=341.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:39", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.354s, obs=0.000s, fwd=0.332s (controller: penin=0.0ms, fwd=332.2ms, darwin=0.2ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:40", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.397s, obs=0.000s, fwd=0.375s (controller: penin=0.0ms, fwd=375.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:49:40", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.412s, obs=0.000s, fwd=0.391s (controller: penin=0.0ms, fwd=390.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:23", "logger": "unified_brain", "level": "ERROR", "message": "Error: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.", "module": "brain_daemon_real_env", "func": "run", "line": 1433}
Traceback (most recent call last):
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 1426, in run
    brain_logger.info("Optimizations:")
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 619, in run_episode
    loss_value = self.train_on_episode(ep_states, ep_actions, ep_rewards, ep_values, ep_log_probs)
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 980, in train_on_episode
    logits_new, values_new, _ = self.controller.v7_bridge(states_batch)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/UNIFIED_BRAIN/brain_system_integration.py", line 69, in forward
    logits = self.action_head(Zf)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 244, in forward
    input = module(input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.
{"timestamp": "2025-10-04 17:50:23", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.181s, obs=0.000s, fwd=0.157s (controller: penin=0.0ms, fwd=156.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:23", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.189s, obs=0.000s, fwd=0.167s (controller: penin=0.0ms, fwd=159.8ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:23", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.167s, obs=0.000s, fwd=0.145s (controller: penin=0.0ms, fwd=145.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:24", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.211s, obs=0.000s, fwd=0.189s (controller: penin=0.0ms, fwd=188.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:24", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.174s, obs=0.000s, fwd=0.153s (controller: penin=0.0ms, fwd=153.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:24", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.194s, obs=0.000s, fwd=0.173s (controller: penin=0.0ms, fwd=172.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:24", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.190s, obs=0.000s, fwd=0.167s (controller: penin=0.0ms, fwd=133.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:24", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.234s, obs=0.000s, fwd=0.212s (controller: penin=0.0ms, fwd=212.1ms, darwin=0.3ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:25", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.207s, obs=0.000s, fwd=0.186s (controller: penin=0.0ms, fwd=185.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:25", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.226s, obs=0.000s, fwd=0.198s (controller: penin=0.0ms, fwd=198.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:25", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.237s, obs=0.000s, fwd=0.206s (controller: penin=0.0ms, fwd=205.4ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:25", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.224s, obs=0.000s, fwd=0.203s (controller: penin=0.0ms, fwd=169.5ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:25", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.193s, obs=0.000s, fwd=0.172s (controller: penin=0.0ms, fwd=171.5ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:26", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.194s, obs=0.000s, fwd=0.173s (controller: penin=0.0ms, fwd=172.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:26", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.184s, obs=0.000s, fwd=0.162s (controller: penin=0.0ms, fwd=162.2ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:26", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.175s, obs=0.000s, fwd=0.150s (controller: penin=0.0ms, fwd=149.8ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:46", "logger": "unified_brain", "level": "ERROR", "message": "Error: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.", "module": "brain_daemon_real_env", "func": "run", "line": 1433}
Traceback (most recent call last):
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 1426, in run
    brain_logger.info("Optimizations:")
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 619, in run_episode
    loss_value = self.train_on_episode(ep_states, ep_actions, ep_rewards, ep_values, ep_log_probs)
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 980, in train_on_episode
    logits_new, values_new, _ = self.controller.v7_bridge(states_batch)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/UNIFIED_BRAIN/brain_system_integration.py", line 69, in forward
    logits = self.action_head(Zf)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 244, in forward
    input = module(input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.
{"timestamp": "2025-10-04 17:50:46", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.271s, obs=0.000s, fwd=0.248s (controller: penin=0.0ms, fwd=237.2ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:46", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.320s, obs=0.000s, fwd=0.296s (controller: penin=0.0ms, fwd=295.3ms, darwin=0.3ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:46", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.282s, obs=0.000s, fwd=0.260s (controller: penin=0.0ms, fwd=260.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:47", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.235s, obs=0.000s, fwd=0.214s (controller: penin=0.0ms, fwd=213.5ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:47", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.230s, obs=0.000s, fwd=0.208s (controller: penin=0.0ms, fwd=208.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:47", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.347s, obs=0.000s, fwd=0.325s (controller: penin=0.0ms, fwd=304.5ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:48", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.511s, obs=0.000s, fwd=0.489s (controller: penin=0.0ms, fwd=489.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:48", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.432s, obs=0.000s, fwd=0.410s (controller: penin=0.0ms, fwd=410.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:49", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.304s, obs=0.000s, fwd=0.278s (controller: penin=0.0ms, fwd=277.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:49", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.300s, obs=0.000s, fwd=0.279s (controller: penin=0.0ms, fwd=278.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:49", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.306s, obs=0.000s, fwd=0.285s (controller: penin=0.0ms, fwd=246.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:49", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.271s, obs=0.000s, fwd=0.249s (controller: penin=0.0ms, fwd=249.2ms, darwin=0.2ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:50", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.276s, obs=0.000s, fwd=0.252s (controller: penin=0.0ms, fwd=251.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:50", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.248s, obs=0.000s, fwd=0.224s (controller: penin=0.0ms, fwd=224.4ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:50", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.282s, obs=0.000s, fwd=0.261s (controller: penin=0.0ms, fwd=260.5ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:50", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.271s, obs=0.000s, fwd=0.250s (controller: penin=0.0ms, fwd=233.8ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:51", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.209s, obs=0.000s, fwd=0.187s (controller: penin=0.0ms, fwd=186.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:51", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.203s, obs=0.000s, fwd=0.182s (controller: penin=0.0ms, fwd=182.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:51", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.195s, obs=0.000s, fwd=0.174s (controller: penin=0.0ms, fwd=173.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:50:51", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.175s, obs=0.000s, fwd=0.153s (controller: penin=0.0ms, fwd=153.2ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:22", "logger": "unified_brain", "level": "ERROR", "message": "Error: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.", "module": "brain_daemon_real_env", "func": "run", "line": 1433}
Traceback (most recent call last):
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 1426, in run
    brain_logger.info("Optimizations:")
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 619, in run_episode
    loss_value = self.train_on_episode(ep_states, ep_actions, ep_rewards, ep_values, ep_log_probs)
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 980, in train_on_episode
    logits_new, values_new, _ = self.controller.v7_bridge(states_batch)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/UNIFIED_BRAIN/brain_system_integration.py", line 69, in forward
    logits = self.action_head(Zf)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 244, in forward
    input = module(input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.
{"timestamp": "2025-10-04 17:51:23", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.161s, obs=0.000s, fwd=0.139s (controller: penin=0.0ms, fwd=120.8ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:23", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.190s, obs=0.000s, fwd=0.169s (controller: penin=0.0ms, fwd=168.0ms, darwin=0.7ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:23", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.213s, obs=0.000s, fwd=0.192s (controller: penin=0.0ms, fwd=192.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:23", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.242s, obs=0.000s, fwd=0.219s (controller: penin=0.0ms, fwd=218.5ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:23", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.133s, obs=0.000s, fwd=0.111s (controller: penin=0.0ms, fwd=111.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:23", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.186s, obs=0.000s, fwd=0.164s (controller: penin=0.0ms, fwd=134.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:24", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.143s, obs=0.000s, fwd=0.121s (controller: penin=0.0ms, fwd=121.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:24", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.170s, obs=0.000s, fwd=0.147s (controller: penin=0.0ms, fwd=147.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:24", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.167s, obs=0.000s, fwd=0.146s (controller: penin=0.0ms, fwd=145.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:24", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.187s, obs=0.000s, fwd=0.161s (controller: penin=0.0ms, fwd=161.2ms, darwin=0.0ms), act=0.005s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:24", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.181s, obs=0.000s, fwd=0.160s (controller: penin=0.0ms, fwd=150.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:24", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.125s, obs=0.000s, fwd=0.103s (controller: penin=0.0ms, fwd=103.0ms, darwin=0.4ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:40", "logger": "unified_brain", "level": "ERROR", "message": "Error: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.", "module": "brain_daemon_real_env", "func": "run", "line": 1433}
Traceback (most recent call last):
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 1426, in run
    brain_logger.info("Optimizations:")
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 619, in run_episode
    loss_value = self.train_on_episode(ep_states, ep_actions, ep_rewards, ep_values, ep_log_probs)
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 980, in train_on_episode
    logits_new, values_new, _ = self.controller.v7_bridge(states_batch)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/UNIFIED_BRAIN/brain_system_integration.py", line 69, in forward
    logits = self.action_head(Zf)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 244, in forward
    input = module(input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.
{"timestamp": "2025-10-04 17:51:40", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.147s, obs=0.000s, fwd=0.123s (controller: penin=0.0ms, fwd=123.2ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:40", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.164s, obs=0.000s, fwd=0.140s (controller: penin=0.0ms, fwd=139.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:41", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.289s, obs=0.000s, fwd=0.267s (controller: penin=0.0ms, fwd=267.4ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:41", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.182s, obs=0.000s, fwd=0.160s (controller: penin=0.0ms, fwd=143.3ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:41", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.182s, obs=0.000s, fwd=0.161s (controller: penin=0.0ms, fwd=161.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:41", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.174s, obs=0.000s, fwd=0.150s (controller: penin=0.0ms, fwd=149.8ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:41", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.140s, obs=0.000s, fwd=0.119s (controller: penin=0.0ms, fwd=118.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:42", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.168s, obs=0.000s, fwd=0.147s (controller: penin=0.0ms, fwd=146.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:42", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.163s, obs=0.000s, fwd=0.141s (controller: penin=0.0ms, fwd=123.2ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:42", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.219s, obs=0.000s, fwd=0.197s (controller: penin=0.0ms, fwd=196.9ms, darwin=0.2ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:42", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.179s, obs=0.000s, fwd=0.158s (controller: penin=0.0ms, fwd=158.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:42", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.162s, obs=0.000s, fwd=0.141s (controller: penin=0.0ms, fwd=140.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:42", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.181s, obs=0.000s, fwd=0.160s (controller: penin=0.0ms, fwd=159.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:43", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.176s, obs=0.000s, fwd=0.155s (controller: penin=0.0ms, fwd=143.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:43", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.171s, obs=0.000s, fwd=0.149s (controller: penin=0.0ms, fwd=149.4ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:43", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.166s, obs=0.000s, fwd=0.144s (controller: penin=0.0ms, fwd=144.2ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:43", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.123s, obs=0.000s, fwd=0.101s (controller: penin=0.0ms, fwd=101.2ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:43", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.179s, obs=0.000s, fwd=0.158s (controller: penin=0.0ms, fwd=157.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:43", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.222s, obs=0.000s, fwd=0.200s (controller: penin=0.0ms, fwd=170.2ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:44", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.206s, obs=0.000s, fwd=0.184s (controller: penin=0.0ms, fwd=183.9ms, darwin=0.2ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:44", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.201s, obs=0.000s, fwd=0.177s (controller: penin=0.0ms, fwd=176.8ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:44", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.174s, obs=0.000s, fwd=0.151s (controller: penin=0.0ms, fwd=151.4ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:44", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.288s, obs=0.000s, fwd=0.267s (controller: penin=0.0ms, fwd=266.8ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:51:45", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.248s, obs=0.000s, fwd=0.227s (controller: penin=0.0ms, fwd=217.5ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:52:51", "logger": "unified_brain", "level": "ERROR", "message": "Error: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.", "module": "brain_daemon_real_env", "func": "run", "line": 1433}
Traceback (most recent call last):
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 1426, in run
    brain_logger.info("Optimizations:")
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 619, in run_episode
    loss_value = self.train_on_episode(ep_states, ep_actions, ep_rewards, ep_values, ep_log_probs)
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 980, in train_on_episode
    logits_new, values_new, _ = self.controller.v7_bridge(states_batch)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/UNIFIED_BRAIN/brain_system_integration.py", line 69, in forward
    logits = self.action_head(Zf)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 244, in forward
    input = module(input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.
{"timestamp": "2025-10-04 17:52:52", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.359s, obs=0.000s, fwd=0.331s (controller: penin=0.0ms, fwd=330.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:52:52", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.424s, obs=0.000s, fwd=0.397s (controller: penin=0.0ms, fwd=396.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:52:53", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.408s, obs=0.000s, fwd=0.378s (controller: penin=0.0ms, fwd=377.5ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:52:53", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.342s, obs=0.000s, fwd=0.311s (controller: penin=0.0ms, fwd=310.8ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:52:53", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.602s, obs=0.000s, fwd=0.558s (controller: penin=0.0ms, fwd=453.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:52:54", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.519s, obs=0.000s, fwd=0.494s (controller: penin=0.0ms, fwd=493.7ms, darwin=0.2ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:52:54", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.506s, obs=0.000s, fwd=0.468s (controller: penin=0.0ms, fwd=468.3ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:52:55", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.445s, obs=0.000s, fwd=0.409s (controller: penin=0.0ms, fwd=408.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:52:55", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.484s, obs=0.000s, fwd=0.460s (controller: penin=0.0ms, fwd=459.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:52:56", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.490s, obs=0.000s, fwd=0.456s (controller: penin=0.0ms, fwd=439.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:52:56", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.455s, obs=0.000s, fwd=0.431s (controller: penin=0.0ms, fwd=430.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:52:57", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.423s, obs=0.000s, fwd=0.398s (controller: penin=0.0ms, fwd=397.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:36", "logger": "unified_brain", "level": "ERROR", "message": "Error: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.", "module": "brain_daemon_real_env", "func": "run", "line": 1433}
Traceback (most recent call last):
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 1426, in run
    brain_logger.info("Optimizations:")
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 619, in run_episode
    loss_value = self.train_on_episode(ep_states, ep_actions, ep_rewards, ep_values, ep_log_probs)
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 980, in train_on_episode
    logits_new, values_new, _ = self.controller.v7_bridge(states_batch)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/UNIFIED_BRAIN/brain_system_integration.py", line 69, in forward
    logits = self.action_head(Zf)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 244, in forward
    input = module(input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.
{"timestamp": "2025-10-04 17:53:36", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.361s, obs=0.000s, fwd=0.340s (controller: penin=0.0ms, fwd=339.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:36", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.362s, obs=0.000s, fwd=0.337s (controller: penin=0.0ms, fwd=336.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:37", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.398s, obs=0.000s, fwd=0.377s (controller: penin=0.0ms, fwd=345.2ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:37", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.348s, obs=0.000s, fwd=0.324s (controller: penin=0.0ms, fwd=324.1ms, darwin=0.2ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:37", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.421s, obs=0.000s, fwd=0.393s (controller: penin=0.0ms, fwd=393.2ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:38", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.363s, obs=0.000s, fwd=0.338s (controller: penin=0.0ms, fwd=338.2ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:38", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.358s, obs=0.000s, fwd=0.334s (controller: penin=0.0ms, fwd=334.4ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:39", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.420s, obs=0.000s, fwd=0.394s (controller: penin=0.0ms, fwd=374.8ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:39", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.353s, obs=0.000s, fwd=0.326s (controller: penin=0.0ms, fwd=325.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:39", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.393s, obs=0.000s, fwd=0.357s (controller: penin=0.0ms, fwd=357.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:40", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.359s, obs=0.000s, fwd=0.334s (controller: penin=0.0ms, fwd=333.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:40", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.356s, obs=0.000s, fwd=0.330s (controller: penin=0.0ms, fwd=329.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:41", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.516s, obs=0.000s, fwd=0.491s (controller: penin=0.0ms, fwd=395.4ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:41", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.479s, obs=0.000s, fwd=0.457s (controller: penin=0.0ms, fwd=456.2ms, darwin=0.2ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:41", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.406s, obs=0.000s, fwd=0.384s (controller: penin=0.0ms, fwd=383.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:42", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.443s, obs=0.000s, fwd=0.421s (controller: penin=0.0ms, fwd=421.3ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:43", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.715s, obs=0.000s, fwd=0.679s (controller: penin=0.0ms, fwd=678.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:43", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.524s, obs=0.000s, fwd=0.499s (controller: penin=0.0ms, fwd=473.3ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:44", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.480s, obs=0.000s, fwd=0.457s (controller: penin=0.0ms, fwd=456.5ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:44", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.511s, obs=0.000s, fwd=0.489s (controller: penin=0.0ms, fwd=488.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:45", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.632s, obs=0.000s, fwd=0.601s (controller: penin=0.0ms, fwd=600.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:45", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.369s, obs=0.000s, fwd=0.347s (controller: penin=0.0ms, fwd=347.2ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:45", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.397s, obs=0.000s, fwd=0.375s (controller: penin=0.0ms, fwd=343.3ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:46", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.328s, obs=0.000s, fwd=0.302s (controller: penin=0.0ms, fwd=301.6ms, darwin=0.2ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:46", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.421s, obs=0.000s, fwd=0.386s (controller: penin=0.0ms, fwd=385.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:47", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.528s, obs=0.000s, fwd=0.506s (controller: penin=0.0ms, fwd=506.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:47", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.425s, obs=0.000s, fwd=0.403s (controller: penin=0.0ms, fwd=403.3ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:48", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.533s, obs=0.000s, fwd=0.511s (controller: penin=0.0ms, fwd=471.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:48", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.394s, obs=0.000s, fwd=0.351s (controller: penin=0.0ms, fwd=351.4ms, darwin=0.0ms), act=0.012s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:49", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.433s, obs=0.000s, fwd=0.400s (controller: penin=0.0ms, fwd=399.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:49", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.602s, obs=0.000s, fwd=0.574s (controller: penin=0.0ms, fwd=573.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:50", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.466s, obs=0.000s, fwd=0.444s (controller: penin=0.0ms, fwd=444.4ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:50", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.472s, obs=0.000s, fwd=0.416s (controller: penin=0.0ms, fwd=406.7ms, darwin=0.0ms), act=0.001s, env=0.030s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:51", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.439s, obs=0.000s, fwd=0.413s (controller: penin=0.0ms, fwd=412.3ms, darwin=0.3ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:51", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.411s, obs=0.000s, fwd=0.388s (controller: penin=0.0ms, fwd=387.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:51", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.422s, obs=0.000s, fwd=0.398s (controller: penin=0.0ms, fwd=398.2ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:52", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.441s, obs=0.000s, fwd=0.404s (controller: penin=0.0ms, fwd=403.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 17:53:52", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=0.410s, obs=0.000s, fwd=0.388s (controller: penin=0.0ms, fwd=320.5ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:01:31", "logger": "unified_brain", "level": "ERROR", "message": "Error: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.", "module": "brain_daemon_real_env", "func": "run", "line": 1433}
Traceback (most recent call last):
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 1426, in run
    brain_logger.info("Optimizations:")
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 619, in run_episode
    loss_value = self.train_on_episode(ep_states, ep_actions, ep_rewards, ep_values, ep_log_probs)
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 980, in train_on_episode
    logits_new, values_new, _ = self.controller.v7_bridge(states_batch)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/UNIFIED_BRAIN/brain_system_integration.py", line 69, in forward
    logits = self.action_head(Zf)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 244, in forward
    input = module(input)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.
{"timestamp": "2025-10-04 18:01:34", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.775s, obs=0.000s, fwd=2.740s (controller: penin=0.0ms, fwd=2740.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:01:36", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=1.903s, obs=0.000s, fwd=1.812s (controller: penin=0.0ms, fwd=1811.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:01:39", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=3.151s, obs=0.000s, fwd=3.068s (controller: penin=0.0ms, fwd=3067.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:01:42", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.652s, obs=0.000s, fwd=2.599s (controller: penin=0.0ms, fwd=2599.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:01:45", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.676s, obs=0.000s, fwd=2.604s (controller: penin=0.0ms, fwd=2106.4ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:01:48", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=3.323s, obs=0.000s, fwd=3.297s (controller: penin=0.0ms, fwd=3296.7ms, darwin=0.3ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:01:50", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.404s, obs=0.000s, fwd=2.380s (controller: penin=0.0ms, fwd=2379.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:01:53", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.639s, obs=0.000s, fwd=2.618s (controller: penin=0.0ms, fwd=2617.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:01:56", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.913s, obs=0.000s, fwd=2.889s (controller: penin=0.0ms, fwd=2889.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:01:59", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=3.287s, obs=0.000s, fwd=3.243s (controller: penin=0.0ms, fwd=3207.4ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:02", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.572s, obs=0.000s, fwd=2.548s (controller: penin=0.0ms, fwd=2547.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:04", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.598s, obs=0.000s, fwd=2.576s (controller: penin=0.0ms, fwd=2576.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:07", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.788s, obs=0.000s, fwd=2.758s (controller: penin=0.0ms, fwd=2757.8ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:10", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.481s, obs=0.000s, fwd=2.456s (controller: penin=0.0ms, fwd=2455.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:13", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=3.302s, obs=0.000s, fwd=3.238s (controller: penin=0.0ms, fwd=3010.3ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:16", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.511s, obs=0.000s, fwd=2.462s (controller: penin=0.0ms, fwd=2461.3ms, darwin=0.3ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:18", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.776s, obs=0.000s, fwd=2.745s (controller: penin=0.0ms, fwd=2744.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:21", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.436s, obs=0.000s, fwd=2.377s (controller: penin=0.0ms, fwd=2377.1ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:23", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.496s, obs=0.000s, fwd=2.462s (controller: penin=0.0ms, fwd=2461.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:27", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=3.411s, obs=0.000s, fwd=3.371s (controller: penin=0.0ms, fwd=3235.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:30", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.867s, obs=0.000s, fwd=2.832s (controller: penin=0.0ms, fwd=2831.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:32", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.259s, obs=0.000s, fwd=2.170s (controller: penin=0.0ms, fwd=2170.2ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:35", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.887s, obs=0.000s, fwd=2.739s (controller: penin=0.0ms, fwd=2738.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:38", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.799s, obs=0.000s, fwd=2.770s (controller: penin=0.0ms, fwd=2769.7ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:40", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.890s, obs=0.000s, fwd=2.859s (controller: penin=0.0ms, fwd=2811.5ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:43", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.784s, obs=0.000s, fwd=2.763s (controller: penin=0.0ms, fwd=2762.5ms, darwin=0.3ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:46", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=3.048s, obs=0.000s, fwd=2.980s (controller: penin=0.0ms, fwd=2979.6ms, darwin=0.0ms), act=0.014s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:49", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=3.079s, obs=0.000s, fwd=2.955s (controller: penin=0.0ms, fwd=2955.0ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:52", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=3.086s, obs=0.000s, fwd=3.057s (controller: penin=0.0ms, fwd=3056.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:55", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=3.016s, obs=0.000s, fwd=2.987s (controller: penin=0.0ms, fwd=2979.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:02:58", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.377s, obs=0.000s, fwd=2.353s (controller: penin=0.0ms, fwd=2352.6ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:03:00", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.066s, obs=0.000s, fwd=2.040s (controller: penin=0.0ms, fwd=2039.9ms, darwin=0.0ms), act=0.005s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:03:03", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.663s, obs=0.000s, fwd=2.594s (controller: penin=0.0ms, fwd=2593.5ms, darwin=0.0ms), act=0.001s, env=0.009s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:03:05", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.912s, obs=0.000s, fwd=2.888s (controller: penin=0.0ms, fwd=2887.9ms, darwin=0.0ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:03:08", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=2.254s, obs=0.000s, fwd=2.150s (controller: penin=0.0ms, fwd=2145.7ms, darwin=0.0ms), act=0.002s, env=0.007s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:03:09", "logger": "unified_brain", "level": "INFO", "message": "ðŸ§ª step_probe: total=1.737s, obs=0.000s, fwd=1.712s (controller: penin=0.0ms, fwd=1711.6ms, darwin=0.3ms), act=0.001s, env=0.000s", "module": "brain_daemon_real_env", "func": "run_episode", "line": 571}
{"timestamp": "2025-10-04 18:07:06", "logger": "unified_brain", "level": "INFO", "message": "Shutdown. Eps: 0, Best: 0.0", "module": "brain_daemon_real_env", "func": "shutdown", "line": 268}
{"timestamp": "2025-10-04 18:07:06", "logger": "unified_brain", "level": "INFO", "message": "ðŸ’¾ Checkpoint: ep 0", "module": "brain_daemon_real_env", "func": "save_checkpoint", "line": 1360}
{"timestamp": "2025-10-04 18:07:06", "logger": "unified_brain", "level": "INFO", "message": "ðŸ”“ Lock released", "module": "brain_daemon_real_env", "func": "_release_lock", "line": 263}
âœ“ Incompletude Infinita carregada automaticamente
  File "/root/UNIFIED_BRAIN/brain_daemon_real_env.py", line 890
    coherence_val = 0.5  # default
    ^^^^^^^^^^^^^
SyntaxError: expected 'except' or 'finally' block
